{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: You are using pip version 20.3.1; however, version 21.0.1 is available.\r\n",
      "You should consider upgrading via the '/Users/kapilan/githome/funny-headlines-nlp/env/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from approach_1 import A1_FFNN_GloVe, A1_CNN_GloVe, A1_CNN_Concat_GloVe, preprocessing_experiment, A1_BERT\n",
    "from approach_2 import A2_Experiments, A2_FFNN_Word2Index, A2_CNN_Word2Index, A2_CNN_Concat_Word2Index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1\n",
    "Run each cell separately to view the training output of each experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading prerequisite word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-26 15:02:19--  http://nlp.stanford.edu/data/glove.6B.zip\r\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\r\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\r\n",
      "--2021-02-26 15:02:19--  https://nlp.stanford.edu/data/glove.6B.zip\r\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\r\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\r\n",
      "--2021-02-26 15:02:20--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\r\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\r\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 862182613 (822M) [application/zip]\r\n",
      "Saving to: ‘glove.6B.zip’\r\n",
      "\r\n",
      "glove.6B.zip        100%[===================>] 822.24M  2.13MB/s    in 6m 59s  \r\n",
      "\r\n",
      "2021-02-26 15:09:20 (1.96 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\r\n",
      "\r\n",
      "Archive:  glove.6B.zip\r\n",
      "  inflating: glove.6B.50d.txt        \r\n",
      "  inflating: glove.6B.100d.txt       \r\n",
      "  inflating: glove.6B.200d.txt       \r\n",
      "  inflating: glove.6B.300d.txt       \r\n"
     ]
    }
   ],
   "source": [
    "!wget -nc http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-26 15:09:36--  http://nlp.stanford.edu/data/glove.twitter.27B.zip\r\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\r\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: https://nlp.stanford.edu/data/glove.twitter.27B.zip [following]\r\n",
      "--2021-02-26 15:09:37--  https://nlp.stanford.edu/data/glove.twitter.27B.zip\r\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\r\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip [following]\r\n",
      "--2021-02-26 15:09:37--  http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip\r\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\r\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 1520408563 (1.4G) [application/zip]\r\n",
      "Saving to: ‘glove.twitter.27B.zip’\r\n",
      "\r\n",
      "glove.twitter.27B.z 100%[===================>]   1.42G  2.19MB/s    in 12m 35s \r\n",
      "\r\n",
      "2021-02-26 15:22:12 (1.92 MB/s) - ‘glove.twitter.27B.zip’ saved [1520408563/1520408563]\r\n",
      "\r\n",
      "Archive:  glove.twitter.27B.zip\r\n",
      "  inflating: glove.twitter.27B.25d.txt  \r\n",
      "  inflating: glove.twitter.27B.50d.txt  \r\n",
      "  inflating: glove.twitter.27B.100d.txt  \r\n",
      "  inflating: glove.twitter.27B.200d.txt  \r\n"
     ]
    }
   ],
   "source": [
    "!wget -nc http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "!unzip glove.twitter.27B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Preprocessing Experiment\n",
    "This experiment takes you through the pre-processing pipeline step by step and aims to reproduce\n",
    "the result shown in the diagram provided in the report.\n",
    "\n",
    "The diagram in the report is based on GloVe 6B embeddings, but you can run it with GloVe Twitter embeddings as well by simply\n",
    "providing the path to the embeddings text file as shown below.\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:10<00:00, 36958.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words not found in GloVe Embeddings after - spliting on whitespace: 396\n",
      "Words not found in GloVe Embeddings after - splitting on hyphens: 216\n",
      "Words not found in GloVe Embeddings after - removing digits: 208\n",
      "Words not found in GloVe Embeddings after - removing stopwords: 208\n",
      "Words not found in GloVe Embeddings after - lemmatizing: 201\n",
      "Words not found in GloVe Embeddings after - stemming: 2243\n"
     ]
    }
   ],
   "source": [
    "preprocessing_experiment.run_this_experiment('glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1193515/1193515 [00:34<00:00, 34950.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words not found in GloVe Embeddings after - spliting on whitespace: 836\n",
      "Words not found in GloVe Embeddings after - splitting on hyphens: 617\n",
      "Words not found in GloVe Embeddings after - removing digits: 426\n",
      "Words not found in GloVe Embeddings after - removing stopwords: 426\n",
      "Words not found in GloVe Embeddings after - lemmatizing: 373\n",
      "Words not found in GloVe Embeddings after - stemming: 1646\n"
     ]
    }
   ],
   "source": [
    "preprocessing_experiment.run_this_experiment('glove.twitter.27B.100d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide on the preprocessing pipeline to use for the rest of this notebook based on the results above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first built a simple Feed Forward Neural Network using pretrained 100-dimensional GloVe embeddings.\n",
    "Our network was designed with an Embedding layer which would take the input headlines (in index form computed by\n",
    "converting words to indices) and would perform a lookup by using this index to get the corresponding GloVe vector.\n",
    "\n",
    "We account for the varying number of words in the input sentences by normalising our inputs with the sentence lengths.\n",
    "In our network we use 3 linear layers with ReLU activations followed by an output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GloVe Experiments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model-downloads/glove.6B.100d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-9-c84315974d18>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mA1_FFNN_GloVe\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_this_experiment\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/githome/funny-headlines-nlp/approach_1/A1_FFNN_GloVe.py\u001B[0m in \u001B[0;36mrun_this_experiment\u001B[0;34m()\u001B[0m\n\u001B[1;32m     65\u001B[0m                                                     collate_fn=collate_fn_padd)\n\u001B[1;32m     66\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 67\u001B[0;31m     \u001B[0mglove_tensor\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwords_not_in_glove\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbuild_embedding_tensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjoint_vocab\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m100\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     68\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m     \u001B[0mEMBEDDING_DIM\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m100\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/githome/funny-headlines-nlp/utils/processor.py\u001B[0m in \u001B[0;36mbuild_embedding_tensor\u001B[0;34m(vocab, embedding_dim)\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mbuild_embedding_tensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvocab\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0membedding_dim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m50\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     63\u001B[0m     \u001B[0mglove_vectors\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvocab\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0membedding_dim\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 64\u001B[0;31m     \u001B[0mword2embedding\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbuild_glove_dictionary\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0membedding_dim\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     65\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     66\u001B[0m     \u001B[0mwords_not_in_glove\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/githome/funny-headlines-nlp/utils/processor.py\u001B[0m in \u001B[0;36mbuild_glove_dictionary\u001B[0;34m(embedding_dim)\u001B[0m\n\u001B[1;32m     45\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mbuild_glove_dictionary\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0membedding_dim\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m     \u001B[0mword2embedding\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 47\u001B[0;31m     \u001B[0;32mwith\u001B[0m \u001B[0mcodecs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'model-downloads/glove.6B.{}d.txt'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0membedding_dim\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'r'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'utf-8'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     48\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mline\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreadlines\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     49\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mline\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstrip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m3\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/Cellar/python@3.9/3.9.1_8/Frameworks/Python.framework/Versions/3.9/lib/python3.9/codecs.py\u001B[0m in \u001B[0;36mopen\u001B[0;34m(filename, mode, encoding, errors, buffering)\u001B[0m\n\u001B[1;32m    903\u001B[0m         \u001B[0;31m# Force opening of the file in binary mode\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    904\u001B[0m         \u001B[0mmode\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmode\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'b'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 905\u001B[0;31m     \u001B[0mfile\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbuiltins\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbuffering\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    906\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mencoding\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    907\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mfile\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'model-downloads/glove.6B.100d.txt'"
     ]
    }
   ],
   "source": [
    "A1_FFNN_GloVe.run_this_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then built CNN models and again make use of an embedding layer with the pre-trained GloVe embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We first use a simple CNN structure which consists of a 2D convolutional layer with ReLU activation.\n",
    "We consider 2 words at a time by using a 2 x 100 window size during convolution.\n",
    "This is followed by a 1D MaxPool on the feature map with dropout, then a fully connected layer with dropout and\n",
    "finally another fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "A1_CNN_GloVe.run_this_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Another more complicated CNN was built which utilised parallel convolutions and GloVe.\n",
    "We took 3 convolutions in parallel (i.e. each one on the original input vector) of varying window sizes considering\n",
    "different number of words. Each parallel branch was followed by a ReLU activation and then a 1D Max Pool operation.\n",
    "The 3 parallel outputs were then concatenated into 1 tensor with dropout.\n",
    "This was passed into a fully connected layer with dropout and then a second fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "A1_CNN_Concat_GloVe.run_this_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "A1_BERT.run_this_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Approach 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Self-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n",
      "| Epoch: 01 | Train Loss: 1.8325 | Train MSE: 1.8325 | Train RMSE: 1.3537 | \n",
      "         Val. Loss: 1.5678 | Val. MSE: 1.5678 |  Val. RMSE: 1.2521 |\n",
      "| Epoch: 02 | Train Loss: 1.3219 | Train MSE: 1.3219 | Train RMSE: 1.1497 | \n",
      "         Val. Loss: 1.1483 | Val. MSE: 1.1483 |  Val. RMSE: 1.0716 |\n",
      "| Epoch: 03 | Train Loss: 0.9706 | Train MSE: 0.9706 | Train RMSE: 0.9852 | \n",
      "         Val. Loss: 0.8483 | Val. MSE: 0.8483 |  Val. RMSE: 0.9211 |\n",
      "| Epoch: 04 | Train Loss: 0.7249 | Train MSE: 0.7249 | Train RMSE: 0.8514 | \n",
      "         Val. Loss: 0.6430 | Val. MSE: 0.6430 |  Val. RMSE: 0.8019 |\n",
      "| Epoch: 05 | Train Loss: 0.5617 | Train MSE: 0.5617 | Train RMSE: 0.7494 | \n",
      "         Val. Loss: 0.5092 | Val. MSE: 0.5092 |  Val. RMSE: 0.7136 |\n",
      "| Epoch: 06 | Train Loss: 0.4587 | Train MSE: 0.4587 | Train RMSE: 0.6773 | \n",
      "         Val. Loss: 0.4270 | Val. MSE: 0.4270 |  Val. RMSE: 0.6534 |\n",
      "| Epoch: 07 | Train Loss: 0.3991 | Train MSE: 0.3991 | Train RMSE: 0.6317 | \n",
      "         Val. Loss: 0.3809 | Val. MSE: 0.3809 |  Val. RMSE: 0.6172 |\n",
      "| Epoch: 08 | Train Loss: 0.3673 | Train MSE: 0.3673 | Train RMSE: 0.6060 | \n",
      "         Val. Loss: 0.3569 | Val. MSE: 0.3569 |  Val. RMSE: 0.5974 |\n",
      "| Epoch: 09 | Train Loss: 0.3518 | Train MSE: 0.3518 | Train RMSE: 0.5931 | \n",
      "         Val. Loss: 0.3454 | Val. MSE: 0.3454 |  Val. RMSE: 0.5877 |\n",
      "| Epoch: 10 | Train Loss: 0.3453 | Train MSE: 0.3453 | Train RMSE: 0.5876 | \n",
      "         Val. Loss: 0.3405 | Val. MSE: 0.3405 |  Val. RMSE: 0.5835 |\n",
      "| Epoch: 11 | Train Loss: 0.3428 | Train MSE: 0.3428 | Train RMSE: 0.5855 | \n",
      "         Val. Loss: 0.3384 | Val. MSE: 0.3384 |  Val. RMSE: 0.5818 |\n",
      "| Epoch: 12 | Train Loss: 0.3419 | Train MSE: 0.3419 | Train RMSE: 0.5847 | \n",
      "         Val. Loss: 0.3376 | Val. MSE: 0.3376 |  Val. RMSE: 0.5810 |\n",
      "| Epoch: 13 | Train Loss: 0.3416 | Train MSE: 0.3416 | Train RMSE: 0.5845 | \n",
      "         Val. Loss: 0.3373 | Val. MSE: 0.3373 |  Val. RMSE: 0.5807 |\n",
      "| Epoch: 14 | Train Loss: 0.3416 | Train MSE: 0.3416 | Train RMSE: 0.5844 | \n",
      "         Val. Loss: 0.3371 | Val. MSE: 0.3371 |  Val. RMSE: 0.5806 |\n",
      "| Epoch: 15 | Train Loss: 0.3415 | Train MSE: 0.3415 | Train RMSE: 0.5844 | \n",
      "         Val. Loss: 0.3370 | Val. MSE: 0.3370 |  Val. RMSE: 0.5805 |\n",
      "| Epoch: 16 | Train Loss: 0.3412 | Train MSE: 0.3412 | Train RMSE: 0.5841 | \n",
      "         Val. Loss: 0.3369 | Val. MSE: 0.3369 |  Val. RMSE: 0.5805 |\n",
      "| Epoch: 17 | Train Loss: 0.3367 | Train MSE: 0.3367 | Train RMSE: 0.5802 | \n",
      "         Val. Loss: 0.3323 | Val. MSE: 0.3323 |  Val. RMSE: 0.5764 |\n",
      "| Epoch: 18 | Train Loss: 0.3141 | Train MSE: 0.3141 | Train RMSE: 0.5605 | \n",
      "         Val. Loss: 0.3303 | Val. MSE: 0.3303 |  Val. RMSE: 0.5747 |\n",
      "| Epoch: 19 | Train Loss: 0.2882 | Train MSE: 0.2882 | Train RMSE: 0.5369 | \n",
      "         Val. Loss: 0.3315 | Val. MSE: 0.3315 |  Val. RMSE: 0.5758 |\n",
      "| Epoch: 20 | Train Loss: 0.2665 | Train MSE: 0.2665 | Train RMSE: 0.5162 | \n",
      "         Val. Loss: 0.3477 | Val. MSE: 0.3477 |  Val. RMSE: 0.5896 |\n",
      "| Test Set MSE: 0.4028 | RMSE: 0.6347 |\n"
     ]
    }
   ],
   "source": [
    "A2_FFNN_Word2Index.run_this_experiment()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n",
      "| Epoch: 01 | Train Loss: 0.6544 | Train MSE: 0.6544 | Train RMSE: 0.8090 | \n",
      "         Val. Loss: 0.3571 | Val. MSE: 0.3571 |  Val. RMSE: 0.5976 |\n",
      "| Epoch: 02 | Train Loss: 0.3903 | Train MSE: 0.3903 | Train RMSE: 0.6247 | \n",
      "         Val. Loss: 0.3367 | Val. MSE: 0.3367 |  Val. RMSE: 0.5803 |\n",
      "| Epoch: 03 | Train Loss: 0.3570 | Train MSE: 0.3570 | Train RMSE: 0.5975 | \n",
      "         Val. Loss: 0.3340 | Val. MSE: 0.3340 |  Val. RMSE: 0.5779 |\n",
      "| Epoch: 04 | Train Loss: 0.3459 | Train MSE: 0.3459 | Train RMSE: 0.5881 | \n",
      "         Val. Loss: 0.3356 | Val. MSE: 0.3356 |  Val. RMSE: 0.5794 |\n",
      "| Epoch: 05 | Train Loss: 0.3375 | Train MSE: 0.3375 | Train RMSE: 0.5809 | \n",
      "         Val. Loss: 0.3334 | Val. MSE: 0.3334 |  Val. RMSE: 0.5774 |\n",
      "| Epoch: 06 | Train Loss: 0.3235 | Train MSE: 0.3235 | Train RMSE: 0.5687 | \n",
      "         Val. Loss: 0.3335 | Val. MSE: 0.3335 |  Val. RMSE: 0.5775 |\n",
      "| Epoch: 07 | Train Loss: 0.3086 | Train MSE: 0.3086 | Train RMSE: 0.5555 | \n",
      "         Val. Loss: 0.3330 | Val. MSE: 0.3330 |  Val. RMSE: 0.5770 |\n",
      "| Epoch: 08 | Train Loss: 0.2909 | Train MSE: 0.2909 | Train RMSE: 0.5394 | \n",
      "         Val. Loss: 0.3368 | Val. MSE: 0.3368 |  Val. RMSE: 0.5804 |\n",
      "| Epoch: 09 | Train Loss: 0.2705 | Train MSE: 0.2705 | Train RMSE: 0.5201 | \n",
      "         Val. Loss: 0.3396 | Val. MSE: 0.3396 |  Val. RMSE: 0.5827 |\n",
      "| Epoch: 10 | Train Loss: 0.2577 | Train MSE: 0.2577 | Train RMSE: 0.5077 | \n",
      "         Val. Loss: 0.3437 | Val. MSE: 0.3437 |  Val. RMSE: 0.5862 |\n",
      "| Epoch: 11 | Train Loss: 0.2412 | Train MSE: 0.2412 | Train RMSE: 0.4912 | \n",
      "         Val. Loss: 0.3444 | Val. MSE: 0.3444 |  Val. RMSE: 0.5868 |\n",
      "| Epoch: 12 | Train Loss: 0.2321 | Train MSE: 0.2321 | Train RMSE: 0.4818 | \n",
      "         Val. Loss: 0.3548 | Val. MSE: 0.3548 |  Val. RMSE: 0.5956 |\n",
      "| Epoch: 13 | Train Loss: 0.2152 | Train MSE: 0.2152 | Train RMSE: 0.4639 | \n",
      "         Val. Loss: 0.3516 | Val. MSE: 0.3516 |  Val. RMSE: 0.5930 |\n",
      "| Epoch: 14 | Train Loss: 0.2053 | Train MSE: 0.2053 | Train RMSE: 0.4531 | \n",
      "         Val. Loss: 0.3534 | Val. MSE: 0.3534 |  Val. RMSE: 0.5944 |\n",
      "| Epoch: 15 | Train Loss: 0.1941 | Train MSE: 0.1941 | Train RMSE: 0.4406 | \n",
      "         Val. Loss: 0.3672 | Val. MSE: 0.3672 |  Val. RMSE: 0.6060 |\n",
      "| Epoch: 16 | Train Loss: 0.1890 | Train MSE: 0.1890 | Train RMSE: 0.4348 | \n",
      "         Val. Loss: 0.3636 | Val. MSE: 0.3636 |  Val. RMSE: 0.6030 |\n",
      "| Epoch: 17 | Train Loss: 0.1811 | Train MSE: 0.1811 | Train RMSE: 0.4256 | \n",
      "         Val. Loss: 0.3611 | Val. MSE: 0.3611 |  Val. RMSE: 0.6009 |\n",
      "| Epoch: 18 | Train Loss: 0.1718 | Train MSE: 0.1718 | Train RMSE: 0.4145 | \n",
      "         Val. Loss: 0.3571 | Val. MSE: 0.3571 |  Val. RMSE: 0.5976 |\n",
      "| Epoch: 19 | Train Loss: 0.1649 | Train MSE: 0.1649 | Train RMSE: 0.4060 | \n",
      "         Val. Loss: 0.3669 | Val. MSE: 0.3669 |  Val. RMSE: 0.6057 |\n",
      "| Epoch: 20 | Train Loss: 0.1662 | Train MSE: 0.1662 | Train RMSE: 0.4077 | \n",
      "         Val. Loss: 0.3698 | Val. MSE: 0.3698 |  Val. RMSE: 0.6081 |\n",
      "| Test Set MSE: 0.4606 | RMSE: 0.6787 |\n"
     ]
    }
   ],
   "source": [
    "A2_CNN_Word2Index.run_this_experiment()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n",
      "| Epoch: 01 | Train Loss: 0.4567 | Train MSE: 0.4567 | Train RMSE: 0.6758 | \n",
      "         Val. Loss: 0.3437 | Val. MSE: 0.3437 |  Val. RMSE: 0.5862 |\n",
      "| Epoch: 02 | Train Loss: 0.3670 | Train MSE: 0.3670 | Train RMSE: 0.6058 | \n",
      "         Val. Loss: 0.3411 | Val. MSE: 0.3411 |  Val. RMSE: 0.5840 |\n",
      "| Epoch: 03 | Train Loss: 0.3403 | Train MSE: 0.3403 | Train RMSE: 0.5834 | \n",
      "         Val. Loss: 0.3354 | Val. MSE: 0.3354 |  Val. RMSE: 0.5792 |\n",
      "| Epoch: 04 | Train Loss: 0.3271 | Train MSE: 0.3271 | Train RMSE: 0.5719 | \n",
      "         Val. Loss: 0.3353 | Val. MSE: 0.3353 |  Val. RMSE: 0.5791 |\n",
      "| Epoch: 05 | Train Loss: 0.3091 | Train MSE: 0.3091 | Train RMSE: 0.5560 | \n",
      "         Val. Loss: 0.3335 | Val. MSE: 0.3335 |  Val. RMSE: 0.5775 |\n",
      "| Epoch: 06 | Train Loss: 0.2979 | Train MSE: 0.2979 | Train RMSE: 0.5458 | \n",
      "         Val. Loss: 0.3342 | Val. MSE: 0.3342 |  Val. RMSE: 0.5781 |\n",
      "| Epoch: 07 | Train Loss: 0.2758 | Train MSE: 0.2758 | Train RMSE: 0.5252 | \n",
      "         Val. Loss: 0.3337 | Val. MSE: 0.3337 |  Val. RMSE: 0.5776 |\n",
      "| Epoch: 08 | Train Loss: 0.2585 | Train MSE: 0.2585 | Train RMSE: 0.5085 | \n",
      "         Val. Loss: 0.3464 | Val. MSE: 0.3464 |  Val. RMSE: 0.5886 |\n",
      "| Epoch: 09 | Train Loss: 0.2425 | Train MSE: 0.2425 | Train RMSE: 0.4924 | \n",
      "         Val. Loss: 0.3483 | Val. MSE: 0.3483 |  Val. RMSE: 0.5902 |\n",
      "| Epoch: 10 | Train Loss: 0.2199 | Train MSE: 0.2199 | Train RMSE: 0.4689 | \n",
      "         Val. Loss: 0.3450 | Val. MSE: 0.3450 |  Val. RMSE: 0.5874 |\n",
      "| Epoch: 11 | Train Loss: 0.2146 | Train MSE: 0.2146 | Train RMSE: 0.4632 | \n",
      "         Val. Loss: 0.3432 | Val. MSE: 0.3432 |  Val. RMSE: 0.5858 |\n",
      "| Epoch: 12 | Train Loss: 0.2018 | Train MSE: 0.2018 | Train RMSE: 0.4492 | \n",
      "         Val. Loss: 0.3479 | Val. MSE: 0.3479 |  Val. RMSE: 0.5898 |\n",
      "| Epoch: 13 | Train Loss: 0.1873 | Train MSE: 0.1873 | Train RMSE: 0.4328 | \n",
      "         Val. Loss: 0.3501 | Val. MSE: 0.3501 |  Val. RMSE: 0.5917 |\n",
      "| Epoch: 14 | Train Loss: 0.1776 | Train MSE: 0.1776 | Train RMSE: 0.4214 | \n",
      "         Val. Loss: 0.3496 | Val. MSE: 0.3496 |  Val. RMSE: 0.5913 |\n",
      "| Epoch: 15 | Train Loss: 0.1689 | Train MSE: 0.1689 | Train RMSE: 0.4109 | \n",
      "         Val. Loss: 0.3587 | Val. MSE: 0.3587 |  Val. RMSE: 0.5989 |\n",
      "| Epoch: 16 | Train Loss: 0.1631 | Train MSE: 0.1631 | Train RMSE: 0.4038 | \n",
      "         Val. Loss: 0.3532 | Val. MSE: 0.3532 |  Val. RMSE: 0.5943 |\n",
      "| Epoch: 17 | Train Loss: 0.1561 | Train MSE: 0.1561 | Train RMSE: 0.3951 | \n",
      "         Val. Loss: 0.3653 | Val. MSE: 0.3653 |  Val. RMSE: 0.6044 |\n",
      "| Epoch: 18 | Train Loss: 0.1514 | Train MSE: 0.1514 | Train RMSE: 0.3891 | \n",
      "         Val. Loss: 0.3601 | Val. MSE: 0.3601 |  Val. RMSE: 0.6001 |\n",
      "| Epoch: 19 | Train Loss: 0.1390 | Train MSE: 0.1390 | Train RMSE: 0.3728 | \n",
      "         Val. Loss: 0.3672 | Val. MSE: 0.3672 |  Val. RMSE: 0.6060 |\n",
      "| Epoch: 20 | Train Loss: 0.1374 | Train MSE: 0.1374 | Train RMSE: 0.3707 | \n",
      "         Val. Loss: 0.3624 | Val. MSE: 0.3624 |  Val. RMSE: 0.6020 |\n",
      "| Test Set MSE: 0.4944 | RMSE: 0.7031 |\n"
     ]
    }
   ],
   "source": [
    "A2_CNN_Concat_Word2Index.run_this_experiment()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Set MSE: 0.3287 | RMSE: 0.5734 |\n"
     ]
    }
   ],
   "source": [
    "A2_Experiments.run_SVR_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-7-23e7d505fb4c>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mA2_Experiments\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_RandomForestRegressor_experiment\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/githome/funny-headlines-nlp/approach_2/A2_Experiments.py\u001B[0m in \u001B[0;36mrun_RandomForestRegressor_experiment\u001B[0;34m()\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     49\u001B[0m     \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mRandomForestRegressor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 50\u001B[0;31m     \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtraining_bags_of_words\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtraining_gradeset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     51\u001B[0m     \u001B[0mpredictions\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalidation_bag_of_words\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/githome/funny-headlines-nlp/env/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m    385\u001B[0m             \u001B[0;31m# parallel_backend contexts set at a higher level,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    386\u001B[0m             \u001B[0;31m# since correctness does not rely on using threads.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 387\u001B[0;31m             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001B[0m\u001B[1;32m    388\u001B[0m                              \u001B[0;34m**\u001B[0m\u001B[0m_joblib_parallel_args\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprefer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'threads'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    389\u001B[0m                 delayed(_parallel_build_trees)(\n",
      "\u001B[0;32m~/githome/funny-headlines-nlp/env/lib/python3.9/site-packages/joblib/parallel.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1042\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_iterating\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_original_iterator\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1043\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1044\u001B[0;31m             \u001B[0;32mwhile\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdispatch_one_batch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1045\u001B[0m                 \u001B[0;32mpass\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1046\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/githome/funny-headlines-nlp/env/lib/python3.9/site-packages/joblib/parallel.py\u001B[0m in \u001B[0;36mdispatch_one_batch\u001B[0;34m(self, iterator)\u001B[0m\n\u001B[1;32m    857\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    858\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 859\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dispatch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtasks\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    860\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    861\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/githome/funny-headlines-nlp/env/lib/python3.9/site-packages/joblib/parallel.py\u001B[0m in \u001B[0;36m_dispatch\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    775\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    776\u001B[0m             \u001B[0mjob_idx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jobs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 777\u001B[0;31m             \u001B[0mjob\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_backend\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply_async\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcallback\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    778\u001B[0m             \u001B[0;31m# A job can complete so quickly than its callback is\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    779\u001B[0m             \u001B[0;31m# called before we get here, causing self._jobs to\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/githome/funny-headlines-nlp/env/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001B[0m in \u001B[0;36mapply_async\u001B[0;34m(self, func, callback)\u001B[0m\n\u001B[1;32m    206\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mapply_async\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcallback\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    207\u001B[0m         \u001B[0;34m\"\"\"Schedule a func to be run\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 208\u001B[0;31m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mImmediateResult\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    209\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mcallback\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    210\u001B[0m             \u001B[0mcallback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mresult\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/githome/funny-headlines-nlp/env/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    570\u001B[0m         \u001B[0;31m# Don't delay the application, to avoid keeping the input\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    571\u001B[0m         \u001B[0;31m# arguments in memory\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 572\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresults\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbatch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    573\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    574\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/githome/funny-headlines-nlp/env/lib/python3.9/site-packages/joblib/parallel.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    260\u001B[0m         \u001B[0;31m# change the default number of processes to -1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    261\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mparallel_backend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_backend\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_jobs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_n_jobs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 262\u001B[0;31m             return [func(*args, **kwargs)\n\u001B[0m\u001B[1;32m    263\u001B[0m                     for func, args, kwargs in self.items]\n\u001B[1;32m    264\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/githome/funny-headlines-nlp/env/lib/python3.9/site-packages/joblib/parallel.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    260\u001B[0m         \u001B[0;31m# change the default number of processes to -1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    261\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mparallel_backend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_backend\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_jobs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_n_jobs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 262\u001B[0;31m             return [func(*args, **kwargs)\n\u001B[0m\u001B[1;32m    263\u001B[0m                     for func, args, kwargs in self.items]\n\u001B[1;32m    264\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/githome/funny-headlines-nlp/env/lib/python3.9/site-packages/sklearn/utils/fixes.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    220\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__call__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    221\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mconfig_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 222\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunction\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/githome/funny-headlines-nlp/env/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\u001B[0m in \u001B[0;36m_parallel_build_trees\u001B[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001B[0m\n\u001B[1;32m    167\u001B[0m                                                         indices=indices)\n\u001B[1;32m    168\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 169\u001B[0;31m         \u001B[0mtree\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msample_weight\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcurr_sample_weight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcheck_input\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    170\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    171\u001B[0m         \u001B[0mtree\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msample_weight\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msample_weight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcheck_input\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/githome/funny-headlines-nlp/env/lib/python3.9/site-packages/sklearn/tree/_classes.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001B[0m\n\u001B[1;32m   1245\u001B[0m         \"\"\"\n\u001B[1;32m   1246\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1247\u001B[0;31m         super().fit(\n\u001B[0m\u001B[1;32m   1248\u001B[0m             \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1249\u001B[0m             \u001B[0msample_weight\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msample_weight\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/githome/funny-headlines-nlp/env/lib/python3.9/site-packages/sklearn/tree/_classes.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001B[0m\n\u001B[1;32m    387\u001B[0m                                            min_impurity_split)\n\u001B[1;32m    388\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 389\u001B[0;31m         \u001B[0mbuilder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbuild\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtree_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msample_weight\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    390\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    391\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mn_outputs_\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mis_classifier\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "A2_Experiments.run_RandomForestRegressor_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "A2_Experiments.run_LinearRegression_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "A2_Experiments.run_Pipeline_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}