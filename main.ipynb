{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from approach_1 import A1_FFNN_GloVe, A1_CNN_GloVe, A1_CNN_Concat_GloVe, preprocessing_experiment, A1_BERT\n",
    "from approach_2 import A2_Experiments, A2_FFNN_Word2Index, A2_CNN_Word2Index, A2_CNN_Concat_Word2Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING: You are using pip version 20.3.1; however, version 21.0.1 is available.\r\n",
      "You should consider upgrading via the '/Users/kapilan/githome/funny-headlines-nlp/env/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1\n",
    "Run each cell separately to view the training output of each experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading prerequisite word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-26 15:02:19--  http://nlp.stanford.edu/data/glove.6B.zip\r\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\r\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\r\n",
      "--2021-02-26 15:02:19--  https://nlp.stanford.edu/data/glove.6B.zip\r\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\r\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\r\n",
      "--2021-02-26 15:02:20--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\r\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\r\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 862182613 (822M) [application/zip]\r\n",
      "Saving to: ‘glove.6B.zip’\r\n",
      "\r\n",
      "glove.6B.zip        100%[===================>] 822.24M  2.13MB/s    in 6m 59s  \r\n",
      "\r\n",
      "2021-02-26 15:09:20 (1.96 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\r\n",
      "\r\n",
      "Archive:  glove.6B.zip\r\n",
      "  inflating: glove.6B.50d.txt        \r\n",
      "  inflating: glove.6B.100d.txt       \r\n",
      "  inflating: glove.6B.200d.txt       \r\n",
      "  inflating: glove.6B.300d.txt       \r\n"
     ]
    }
   ],
   "source": [
    "!wget -nc http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-26 15:09:36--  http://nlp.stanford.edu/data/glove.twitter.27B.zip\r\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\r\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: https://nlp.stanford.edu/data/glove.twitter.27B.zip [following]\r\n",
      "--2021-02-26 15:09:37--  https://nlp.stanford.edu/data/glove.twitter.27B.zip\r\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\r\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip [following]\r\n",
      "--2021-02-26 15:09:37--  http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip\r\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\r\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 1520408563 (1.4G) [application/zip]\r\n",
      "Saving to: ‘glove.twitter.27B.zip’\r\n",
      "\r\n",
      "glove.twitter.27B.z 100%[===================>]   1.42G  2.19MB/s    in 12m 35s \r\n",
      "\r\n",
      "2021-02-26 15:22:12 (1.92 MB/s) - ‘glove.twitter.27B.zip’ saved [1520408563/1520408563]\r\n",
      "\r\n",
      "Archive:  glove.twitter.27B.zip\r\n",
      "  inflating: glove.twitter.27B.25d.txt  \r\n",
      "  inflating: glove.twitter.27B.50d.txt  \r\n",
      "  inflating: glove.twitter.27B.100d.txt  \r\n",
      "  inflating: glove.twitter.27B.200d.txt  \r\n"
     ]
    }
   ],
   "source": [
    "!wget -nc http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "!unzip glove.twitter.27B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Preprocessing Experiment\n",
    "This experiment takes you through the pre-processing pipeline step by step and aims to reproduce\n",
    "the result shown in the diagram provided in the report.\n",
    "\n",
    "The diagram in the report is based on GloVe 6B embeddings, but you can run it with GloVe Twitter embeddings as well by simply\n",
    "providing the path to the embeddings text file as shown below.\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:10<00:00, 36958.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words not found in GloVe Embeddings after - spliting on whitespace: 396\n",
      "Words not found in GloVe Embeddings after - splitting on hyphens: 216\n",
      "Words not found in GloVe Embeddings after - removing digits: 208\n",
      "Words not found in GloVe Embeddings after - removing stopwords: 208\n",
      "Words not found in GloVe Embeddings after - lemmatizing: 201\n",
      "Words not found in GloVe Embeddings after - stemming: 2243\n"
     ]
    }
   ],
   "source": [
    "preprocessing_experiment.run_this_experiment('glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1193515/1193515 [00:34<00:00, 34950.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words not found in GloVe Embeddings after - spliting on whitespace: 836\n",
      "Words not found in GloVe Embeddings after - splitting on hyphens: 617\n",
      "Words not found in GloVe Embeddings after - removing digits: 426\n",
      "Words not found in GloVe Embeddings after - removing stopwords: 426\n",
      "Words not found in GloVe Embeddings after - lemmatizing: 373\n",
      "Words not found in GloVe Embeddings after - stemming: 1646\n"
     ]
    }
   ],
   "source": [
    "preprocessing_experiment.run_this_experiment('glove.twitter.27B.100d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide on the preprocessing pipeline to use for the rest of this notebook based on the results above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first built a simple Feed Forward Neural Network using pretrained 100-dimensional GloVe embeddings.\n",
    "Our network was designed with an Embedding layer which would take the input headlines (in index form computed by\n",
    "converting words to indices) and would perform a lookup by using this index to get the corresponding GloVe vector.\n",
    "\n",
    "We account for the varying number of words in the input sentences by normalising our inputs with the sentence lengths.\n",
    "In our network we use 3 linear layers with ReLU activations followed by an output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GloVe Experiments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model-downloads/glove.6B.100d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-9-c84315974d18>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mA1_FFNN_GloVe\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_this_experiment\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/githome/funny-headlines-nlp/approach_1/A1_FFNN_GloVe.py\u001B[0m in \u001B[0;36mrun_this_experiment\u001B[0;34m()\u001B[0m\n\u001B[1;32m     65\u001B[0m                                                     collate_fn=collate_fn_padd)\n\u001B[1;32m     66\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 67\u001B[0;31m     \u001B[0mglove_tensor\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwords_not_in_glove\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbuild_embedding_tensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjoint_vocab\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m100\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     68\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m     \u001B[0mEMBEDDING_DIM\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m100\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/githome/funny-headlines-nlp/utils/processor.py\u001B[0m in \u001B[0;36mbuild_embedding_tensor\u001B[0;34m(vocab, embedding_dim)\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mbuild_embedding_tensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvocab\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0membedding_dim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m50\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     63\u001B[0m     \u001B[0mglove_vectors\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvocab\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0membedding_dim\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 64\u001B[0;31m     \u001B[0mword2embedding\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbuild_glove_dictionary\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0membedding_dim\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     65\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     66\u001B[0m     \u001B[0mwords_not_in_glove\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/githome/funny-headlines-nlp/utils/processor.py\u001B[0m in \u001B[0;36mbuild_glove_dictionary\u001B[0;34m(embedding_dim)\u001B[0m\n\u001B[1;32m     45\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mbuild_glove_dictionary\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0membedding_dim\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m     \u001B[0mword2embedding\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 47\u001B[0;31m     \u001B[0;32mwith\u001B[0m \u001B[0mcodecs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'model-downloads/glove.6B.{}d.txt'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0membedding_dim\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'r'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'utf-8'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     48\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mline\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreadlines\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     49\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mline\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstrip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m3\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/Cellar/python@3.9/3.9.1_8/Frameworks/Python.framework/Versions/3.9/lib/python3.9/codecs.py\u001B[0m in \u001B[0;36mopen\u001B[0;34m(filename, mode, encoding, errors, buffering)\u001B[0m\n\u001B[1;32m    903\u001B[0m         \u001B[0;31m# Force opening of the file in binary mode\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    904\u001B[0m         \u001B[0mmode\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmode\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'b'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 905\u001B[0;31m     \u001B[0mfile\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbuiltins\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbuffering\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    906\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mencoding\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    907\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mfile\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'model-downloads/glove.6B.100d.txt'"
     ]
    }
   ],
   "source": [
    "A1_FFNN_GloVe.run_this_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then built CNN models and again make use of an embedding layer with the pre-trained GloVe embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We first use a simple CNN structure which consists of a 2D convolutional layer with ReLU activation.\n",
    "We consider 2 words at a time by using a 2 x 100 window size during convolution.\n",
    "This is followed by a 1D MaxPool on the feature map with dropout, then a fully connected layer with dropout and\n",
    "finally another fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "A1_CNN_GloVe.run_this_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Another more complicated CNN was built which utilised parallel convolutions and GloVe.\n",
    "We took 3 convolutions in parallel (i.e. each one on the original input vector) of varying window sizes considering\n",
    "different number of words. Each parallel branch was followed by a ReLU activation and then a 1D Max Pool operation.\n",
    "The 3 parallel outputs were then concatenated into 1 tensor with dropout.\n",
    "This was passed into a fully connected layer with dropout and then a second fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "A1_CNN_Concat_GloVe.run_this_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "A1_BERT.run_this_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Approach 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Self-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n",
      "| Epoch: 01 | Train Loss: 1.8325 | Train MSE: 0.0575 | Train RMSE: 0.2398 | \n",
      "         Val. Loss: 1.5678 | Val. MSE: 0.0495 |  Val. RMSE: 0.2225 |\n",
      "| Epoch: 02 | Train Loss: 1.3219 | Train MSE: 0.0414 | Train RMSE: 0.2036 | \n",
      "         Val. Loss: 1.1483 | Val. MSE: 0.0363 |  Val. RMSE: 0.1904 |\n",
      "| Epoch: 03 | Train Loss: 0.9706 | Train MSE: 0.0304 | Train RMSE: 0.1743 | \n",
      "         Val. Loss: 0.8483 | Val. MSE: 0.0268 |  Val. RMSE: 0.1637 |\n",
      "| Epoch: 04 | Train Loss: 0.7249 | Train MSE: 0.0227 | Train RMSE: 0.1505 | \n",
      "         Val. Loss: 0.6430 | Val. MSE: 0.0203 |  Val. RMSE: 0.1425 |\n",
      "| Epoch: 05 | Train Loss: 0.5617 | Train MSE: 0.0176 | Train RMSE: 0.1327 | \n",
      "         Val. Loss: 0.5092 | Val. MSE: 0.0161 |  Val. RMSE: 0.1268 |\n",
      "| Epoch: 06 | Train Loss: 0.4587 | Train MSE: 0.0144 | Train RMSE: 0.1198 | \n",
      "         Val. Loss: 0.4270 | Val. MSE: 0.0135 |  Val. RMSE: 0.1161 |\n",
      "| Epoch: 07 | Train Loss: 0.3991 | Train MSE: 0.0125 | Train RMSE: 0.1117 | \n",
      "         Val. Loss: 0.3809 | Val. MSE: 0.0120 |  Val. RMSE: 0.1097 |\n",
      "| Epoch: 08 | Train Loss: 0.3673 | Train MSE: 0.0115 | Train RMSE: 0.1073 | \n",
      "         Val. Loss: 0.3569 | Val. MSE: 0.0113 |  Val. RMSE: 0.1062 |\n",
      "| Epoch: 09 | Train Loss: 0.3518 | Train MSE: 0.0110 | Train RMSE: 0.1050 | \n",
      "         Val. Loss: 0.3454 | Val. MSE: 0.0109 |  Val. RMSE: 0.1045 |\n",
      "| Epoch: 10 | Train Loss: 0.3453 | Train MSE: 0.0108 | Train RMSE: 0.1040 | \n",
      "         Val. Loss: 0.3405 | Val. MSE: 0.0108 |  Val. RMSE: 0.1037 |\n",
      "| Epoch: 11 | Train Loss: 0.3428 | Train MSE: 0.0107 | Train RMSE: 0.1036 | \n",
      "         Val. Loss: 0.3384 | Val. MSE: 0.0107 |  Val. RMSE: 0.1034 |\n",
      "| Epoch: 12 | Train Loss: 0.3419 | Train MSE: 0.0107 | Train RMSE: 0.1035 | \n",
      "         Val. Loss: 0.3376 | Val. MSE: 0.0107 |  Val. RMSE: 0.1033 |\n",
      "| Epoch: 13 | Train Loss: 0.3416 | Train MSE: 0.0107 | Train RMSE: 0.1034 | \n",
      "         Val. Loss: 0.3373 | Val. MSE: 0.0107 |  Val. RMSE: 0.1032 |\n",
      "| Epoch: 14 | Train Loss: 0.3416 | Train MSE: 0.0107 | Train RMSE: 0.1035 | \n",
      "         Val. Loss: 0.3371 | Val. MSE: 0.0107 |  Val. RMSE: 0.1032 |\n",
      "| Epoch: 15 | Train Loss: 0.3415 | Train MSE: 0.0107 | Train RMSE: 0.1034 | \n",
      "         Val. Loss: 0.3370 | Val. MSE: 0.0107 |  Val. RMSE: 0.1032 |\n",
      "| Epoch: 16 | Train Loss: 0.3412 | Train MSE: 0.0107 | Train RMSE: 0.1034 | \n",
      "         Val. Loss: 0.3369 | Val. MSE: 0.0106 |  Val. RMSE: 0.1032 |\n",
      "| Epoch: 17 | Train Loss: 0.3367 | Train MSE: 0.0106 | Train RMSE: 0.1027 | \n",
      "         Val. Loss: 0.3323 | Val. MSE: 0.0105 |  Val. RMSE: 0.1025 |\n",
      "| Epoch: 18 | Train Loss: 0.3141 | Train MSE: 0.0098 | Train RMSE: 0.0991 | \n",
      "         Val. Loss: 0.3303 | Val. MSE: 0.0104 |  Val. RMSE: 0.1022 |\n",
      "| Epoch: 19 | Train Loss: 0.2882 | Train MSE: 0.0090 | Train RMSE: 0.0950 | \n",
      "         Val. Loss: 0.3315 | Val. MSE: 0.0105 |  Val. RMSE: 0.1024 |\n",
      "| Epoch: 20 | Train Loss: 0.2665 | Train MSE: 0.0084 | Train RMSE: 0.0914 | \n",
      "         Val. Loss: 0.3477 | Val. MSE: 0.0110 |  Val. RMSE: 0.1050 |\n",
      "| Test Set MSE: 0.4028 | RMSE: 0.6347 |\n"
     ]
    }
   ],
   "source": [
    "A2_FFNN_Word2Index.run_this_experiment()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n",
      "| Epoch: 01 | Train Loss: 0.6544 | Train MSE: 0.0205 | Train RMSE: 0.1432 |         Val. Loss: 0.3571 | Val. MSE: 0.0113 |  Val. RMSE: 0.1064 |\n",
      "| Epoch: 02 | Train Loss: 0.3903 | Train MSE: 0.0122 | Train RMSE: 0.1106 |         Val. Loss: 0.3367 | Val. MSE: 0.0106 |  Val. RMSE: 0.1032 |\n",
      "| Epoch: 03 | Train Loss: 0.3570 | Train MSE: 0.0112 | Train RMSE: 0.1057 |         Val. Loss: 0.3340 | Val. MSE: 0.0106 |  Val. RMSE: 0.1028 |\n",
      "| Epoch: 04 | Train Loss: 0.3459 | Train MSE: 0.0108 | Train RMSE: 0.1040 |         Val. Loss: 0.3356 | Val. MSE: 0.0106 |  Val. RMSE: 0.1031 |\n",
      "| Epoch: 05 | Train Loss: 0.3375 | Train MSE: 0.0106 | Train RMSE: 0.1028 |         Val. Loss: 0.3334 | Val. MSE: 0.0106 |  Val. RMSE: 0.1027 |\n",
      "| Epoch: 06 | Train Loss: 0.3235 | Train MSE: 0.0102 | Train RMSE: 0.1008 |         Val. Loss: 0.3335 | Val. MSE: 0.0106 |  Val. RMSE: 0.1028 |\n",
      "| Epoch: 07 | Train Loss: 0.3086 | Train MSE: 0.0097 | Train RMSE: 0.0983 |         Val. Loss: 0.3330 | Val. MSE: 0.0106 |  Val. RMSE: 0.1028 |\n",
      "| Epoch: 08 | Train Loss: 0.2909 | Train MSE: 0.0091 | Train RMSE: 0.0955 |         Val. Loss: 0.3368 | Val. MSE: 0.0107 |  Val. RMSE: 0.1033 |\n",
      "| Epoch: 09 | Train Loss: 0.2705 | Train MSE: 0.0085 | Train RMSE: 0.0922 |         Val. Loss: 0.3396 | Val. MSE: 0.0108 |  Val. RMSE: 0.1037 |\n",
      "| Epoch: 10 | Train Loss: 0.2577 | Train MSE: 0.0081 | Train RMSE: 0.0899 |         Val. Loss: 0.3437 | Val. MSE: 0.0109 |  Val. RMSE: 0.1044 |\n",
      "| Epoch: 11 | Train Loss: 0.2412 | Train MSE: 0.0076 | Train RMSE: 0.0869 |         Val. Loss: 0.3444 | Val. MSE: 0.0109 |  Val. RMSE: 0.1044 |\n",
      "| Epoch: 12 | Train Loss: 0.2321 | Train MSE: 0.0073 | Train RMSE: 0.0852 |         Val. Loss: 0.3548 | Val. MSE: 0.0112 |  Val. RMSE: 0.1060 |\n",
      "| Epoch: 13 | Train Loss: 0.2152 | Train MSE: 0.0067 | Train RMSE: 0.0821 |         Val. Loss: 0.3516 | Val. MSE: 0.0112 |  Val. RMSE: 0.1056 |\n",
      "| Epoch: 14 | Train Loss: 0.2053 | Train MSE: 0.0064 | Train RMSE: 0.0802 |         Val. Loss: 0.3534 | Val. MSE: 0.0112 |  Val. RMSE: 0.1059 |\n",
      "| Epoch: 15 | Train Loss: 0.1941 | Train MSE: 0.0061 | Train RMSE: 0.0779 |         Val. Loss: 0.3672 | Val. MSE: 0.0116 |  Val. RMSE: 0.1079 |\n",
      "| Epoch: 16 | Train Loss: 0.1890 | Train MSE: 0.0059 | Train RMSE: 0.0770 |         Val. Loss: 0.3636 | Val. MSE: 0.0115 |  Val. RMSE: 0.1073 |\n",
      "| Epoch: 17 | Train Loss: 0.1811 | Train MSE: 0.0057 | Train RMSE: 0.0753 |         Val. Loss: 0.3611 | Val. MSE: 0.0114 |  Val. RMSE: 0.1069 |\n",
      "| Epoch: 18 | Train Loss: 0.1718 | Train MSE: 0.0054 | Train RMSE: 0.0733 |         Val. Loss: 0.3571 | Val. MSE: 0.0113 |  Val. RMSE: 0.1064 |\n",
      "| Epoch: 19 | Train Loss: 0.1649 | Train MSE: 0.0052 | Train RMSE: 0.0718 |         Val. Loss: 0.3669 | Val. MSE: 0.0116 |  Val. RMSE: 0.1078 |\n",
      "| Epoch: 20 | Train Loss: 0.1662 | Train MSE: 0.0052 | Train RMSE: 0.0722 |         Val. Loss: 0.3698 | Val. MSE: 0.0117 |  Val. RMSE: 0.1083 |\n",
      "| Test Set MSE: 0.4606 | RMSE: 0.6787 |\n"
     ]
    }
   ],
   "source": [
    "A2_CNN_Word2Index.run_this_experiment()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n",
      "| Epoch: 01 | Train Loss: 0.4567 | Train MSE: 0.0143 | Train RMSE: 0.1196 |         Val. Loss: 0.3437 | Val. MSE: 0.0109 |  Val. RMSE: 0.1043 |\n",
      "| Epoch: 02 | Train Loss: 0.3670 | Train MSE: 0.0115 | Train RMSE: 0.1072 |         Val. Loss: 0.3411 | Val. MSE: 0.0108 |  Val. RMSE: 0.1038 |\n",
      "| Epoch: 03 | Train Loss: 0.3403 | Train MSE: 0.0107 | Train RMSE: 0.1033 |         Val. Loss: 0.3354 | Val. MSE: 0.0106 |  Val. RMSE: 0.1029 |\n",
      "| Epoch: 04 | Train Loss: 0.3271 | Train MSE: 0.0102 | Train RMSE: 0.1012 |         Val. Loss: 0.3353 | Val. MSE: 0.0106 |  Val. RMSE: 0.1029 |\n",
      "| Epoch: 05 | Train Loss: 0.3091 | Train MSE: 0.0097 | Train RMSE: 0.0984 |         Val. Loss: 0.3335 | Val. MSE: 0.0105 |  Val. RMSE: 0.1026 |\n",
      "| Epoch: 06 | Train Loss: 0.2979 | Train MSE: 0.0093 | Train RMSE: 0.0966 |         Val. Loss: 0.3342 | Val. MSE: 0.0105 |  Val. RMSE: 0.1027 |\n",
      "| Epoch: 07 | Train Loss: 0.2758 | Train MSE: 0.0087 | Train RMSE: 0.0932 |         Val. Loss: 0.3337 | Val. MSE: 0.0105 |  Val. RMSE: 0.1026 |\n",
      "| Epoch: 08 | Train Loss: 0.2585 | Train MSE: 0.0081 | Train RMSE: 0.0900 |         Val. Loss: 0.3464 | Val. MSE: 0.0109 |  Val. RMSE: 0.1045 |\n",
      "| Epoch: 09 | Train Loss: 0.2425 | Train MSE: 0.0076 | Train RMSE: 0.0872 |         Val. Loss: 0.3483 | Val. MSE: 0.0110 |  Val. RMSE: 0.1048 |\n",
      "| Epoch: 10 | Train Loss: 0.2199 | Train MSE: 0.0069 | Train RMSE: 0.0831 |         Val. Loss: 0.3450 | Val. MSE: 0.0109 |  Val. RMSE: 0.1043 |\n",
      "| Epoch: 11 | Train Loss: 0.2146 | Train MSE: 0.0067 | Train RMSE: 0.0820 |         Val. Loss: 0.3432 | Val. MSE: 0.0108 |  Val. RMSE: 0.1041 |\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-5-71d16f02af94>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mA2_CNN_Concat_Word2Index\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_this_experiment\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/githome/funny-headlines-nlp/approach_2/A2_CNN_Concat_Word2Index.py\u001B[0m in \u001B[0;36mrun_this_experiment\u001B[0;34m()\u001B[0m\n\u001B[1;32m    100\u001B[0m     \u001B[0mloss_fn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mMSELoss\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 102\u001B[0;31m     \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalidation_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m20\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mloss_fn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    103\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    104\u001B[0m     \u001B[0;31m# Test data\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/githome/funny-headlines-nlp/utils/common_utils.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(train_loader, validation_loader, model, number_epoch, optimizer, loss_fn, device)\u001B[0m\n\u001B[1;32m     90\u001B[0m             \u001B[0mepoch_sse\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0msse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     91\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 92\u001B[0;31m         \u001B[0mvalid_loss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalid_mse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m__\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0meval\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalidation_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mloss_fn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     93\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     94\u001B[0m         \u001B[0mepoch_loss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepoch_mse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mepoch_loss\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mno_observations\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepoch_sse\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mno_observations\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/githome/funny-headlines-nlp/utils/common_utils.py\u001B[0m in \u001B[0;36meval\u001B[0;34m(data_iter, model, device, loss_fn)\u001B[0m\n\u001B[1;32m     54\u001B[0m             \u001B[0;31m# We get the mse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     55\u001B[0m             \u001B[0mpred\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpredictions\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 56\u001B[0;31m             \u001B[0msse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel_performance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpred\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     57\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     58\u001B[0m             \u001B[0mepoch_loss\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mtarget\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/githome/funny-headlines-nlp/utils/common_utils.py\u001B[0m in \u001B[0;36mmodel_performance\u001B[0;34m(output, target, print_output)\u001B[0m\n\u001B[1;32m     20\u001B[0m     \u001B[0msq_error\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0moutput\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m**\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 22\u001B[0;31m     \u001B[0mmse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msq_error\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     23\u001B[0m     \u001B[0mrmse\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqrt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<__array_function__ internals>\u001B[0m in \u001B[0;36mmean\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[0;32m~/githome/funny-headlines-nlp/env/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001B[0m in \u001B[0;36mmean\u001B[0;34m(a, axis, dtype, out, keepdims, where)\u001B[0m\n\u001B[1;32m   3417\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mmean\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0maxis\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3418\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3419\u001B[0;31m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001B[0m\u001B[1;32m   3420\u001B[0m                           out=out, **kwargs)\n\u001B[1;32m   3421\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/githome/funny-headlines-nlp/env/lib/python3.9/site-packages/numpy/core/_methods.py\u001B[0m in \u001B[0;36m_mean\u001B[0;34m(a, axis, dtype, out, keepdims, where)\u001B[0m\n\u001B[1;32m    176\u001B[0m             \u001B[0mis_float16_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    177\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 178\u001B[0;31m     \u001B[0mret\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mumr_sum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkeepdims\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwhere\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mwhere\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    179\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mret\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmu\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mndarray\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    180\u001B[0m         ret = um.true_divide(\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "A2_CNN_Concat_Word2Index.run_this_experiment()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| MSE: 0.33 | RMSE: 0.57 |\n"
     ]
    }
   ],
   "source": [
    "A2_Experiments.run_SVR_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| MSE: 0.35 | RMSE: 0.59 |\n"
     ]
    }
   ],
   "source": [
    "A2_Experiments.run_RandomForestRegressor_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| MSE: 43.54 | RMSE: 6.60 |\n"
     ]
    }
   ],
   "source": [
    "A2_Experiments.run_LinearRegression_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-6-f02e9928f64f>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mA2_Experiments\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_Pipeline_experiment\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/githome/funny-headlines-nlp/approach_2/A2_Experiments.py\u001B[0m in \u001B[0;36mrun_Pipeline_experiment\u001B[0;34m()\u001B[0m\n\u001B[1;32m     67\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     68\u001B[0m     \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPipeline\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'poly'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mPolynomialFeatures\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdegree\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;34m'linear'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mLinearRegression\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfit_intercept\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 69\u001B[0;31m     \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtraining_bags_of_words\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtraining_gradeset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     70\u001B[0m     \u001B[0mpredictions\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalidation_bag_of_words\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     71\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/githome/funny-headlines-nlp/env/lib/python3.9/site-packages/sklearn/pipeline.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[1;32m    339\u001B[0m         \"\"\"\n\u001B[1;32m    340\u001B[0m         \u001B[0mfit_params_steps\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_check_fit_params\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mfit_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 341\u001B[0;31m         \u001B[0mXt\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfit_params_steps\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    342\u001B[0m         with _print_elapsed_time('Pipeline',\n\u001B[1;32m    343\u001B[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001B[0;32m~/githome/funny-headlines-nlp/env/lib/python3.9/site-packages/sklearn/pipeline.py\u001B[0m in \u001B[0;36m_fit\u001B[0;34m(self, X, y, **fit_params_steps)\u001B[0m\n\u001B[1;32m    301\u001B[0m                 \u001B[0mcloned_transformer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mclone\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtransformer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    302\u001B[0m             \u001B[0;31m# Fit or load from cache the current transformer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 303\u001B[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001B[0m\u001B[1;32m    304\u001B[0m                 \u001B[0mcloned_transformer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    305\u001B[0m                 \u001B[0mmessage_clsname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'Pipeline'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/githome/funny-headlines-nlp/env/lib/python3.9/site-packages/joblib/memory.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    350\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    351\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__call__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 352\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    353\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    354\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mcall_and_shelve\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/githome/funny-headlines-nlp/env/lib/python3.9/site-packages/sklearn/pipeline.py\u001B[0m in \u001B[0;36m_fit_transform_one\u001B[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001B[0m\n\u001B[1;32m    752\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0m_print_elapsed_time\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmessage_clsname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmessage\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    753\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtransformer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'fit_transform'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 754\u001B[0;31m             \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtransformer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit_transform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfit_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    755\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    756\u001B[0m             \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtransformer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfit_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/githome/funny-headlines-nlp/env/lib/python3.9/site-packages/sklearn/base.py\u001B[0m in \u001B[0;36mfit_transform\u001B[0;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[1;32m    700\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    701\u001B[0m             \u001B[0;31m# fit method of arity 2 (supervised transformation)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 702\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfit_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    703\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    704\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/githome/funny-headlines-nlp/env/lib/python3.9/site-packages/sklearn/preprocessing/_data.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m   1708\u001B[0m                                           self.include_bias)\n\u001B[1;32m   1709\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mn_input_features_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mn_features\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1710\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mn_output_features_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0m_\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcombinations\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1711\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1712\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/githome/funny-headlines-nlp/env/lib/python3.9/site-packages/sklearn/preprocessing/_data.py\u001B[0m in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   1708\u001B[0m                                           self.include_bias)\n\u001B[1;32m   1709\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mn_input_features_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mn_features\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1710\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mn_output_features_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0m_\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcombinations\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1711\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1712\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "A2_Experiments.run_Pipeline_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}