Thu Feb 25 21:08:27 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla T4            On   | 00000000:00:07.0 Off |                    0 |
| N/A   41C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]Downloading:  16%|█▌        | 36.9k/232k [00:00<00:00, 257kB/s]Downloading:  90%|█████████ | 209k/232k [00:00<00:00, 334kB/s] Downloading: 100%|██████████| 232k/232k [00:00<00:00, 774kB/s]
Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]Downloading: 100%|██████████| 433/433 [00:00<00:00, 617kB/s]
Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]Downloading:   0%|          | 2.01M/440M [00:00<00:21, 20.1MB/s]Downloading:   1%|          | 4.19M/440M [00:00<00:21, 20.6MB/s]Downloading:   1%|▏         | 6.29M/440M [00:00<00:21, 19.9MB/s]Downloading:   2%|▏         | 8.00M/440M [00:00<00:23, 18.7MB/s]Downloading:   2%|▏         | 9.76M/440M [00:00<00:24, 17.9MB/s]Downloading:   3%|▎         | 11.7M/440M [00:00<00:23, 18.2MB/s]Downloading:   3%|▎         | 13.8M/440M [00:00<00:22, 19.0MB/s]Downloading:   4%|▎         | 15.8M/440M [00:00<00:22, 19.2MB/s]Downloading:   4%|▍         | 18.0M/440M [00:00<00:21, 19.8MB/s]Downloading:   5%|▍         | 19.9M/440M [00:01<00:21, 19.8MB/s]Downloading:   5%|▍         | 21.9M/440M [00:01<00:21, 19.6MB/s]Downloading:   6%|▌         | 24.3M/440M [00:01<00:20, 20.7MB/s]Downloading:   6%|▌         | 26.5M/440M [00:01<00:19, 20.8MB/s]Downloading:   6%|▋         | 28.6M/440M [00:01<00:19, 20.7MB/s]Downloading:   7%|▋         | 30.8M/440M [00:01<00:19, 21.1MB/s]Downloading:   7%|▋         | 32.9M/440M [00:01<00:21, 18.7MB/s]Downloading:   8%|▊         | 36.0M/440M [00:01<00:19, 21.3MB/s]Downloading:   9%|▊         | 38.3M/440M [00:01<00:20, 19.8MB/s]Downloading:   9%|▉         | 40.4M/440M [00:02<00:19, 20.2MB/s]Downloading:  10%|▉         | 42.5M/440M [00:02<00:19, 20.0MB/s]Downloading:  10%|█         | 44.8M/440M [00:02<00:20, 19.7MB/s]Downloading:  11%|█         | 46.8M/440M [00:02<00:21, 18.1MB/s]Downloading:  11%|█         | 49.4M/440M [00:02<00:19, 19.9MB/s]Downloading:  12%|█▏        | 51.4M/440M [00:02<00:19, 19.8MB/s]Downloading:  12%|█▏        | 53.5M/440M [00:02<00:20, 19.0MB/s]Downloading:  13%|█▎        | 55.8M/440M [00:02<00:19, 19.9MB/s]Downloading:  13%|█▎        | 57.9M/440M [00:02<00:19, 20.1MB/s]Downloading:  14%|█▎        | 59.9M/440M [00:03<00:24, 15.4MB/s]Downloading:  14%|█▍        | 62.9M/440M [00:03<00:21, 17.9MB/s]Downloading:  15%|█▍        | 64.9M/440M [00:03<00:26, 14.4MB/s]Downloading:  15%|█▌        | 66.7M/440M [00:03<00:24, 15.1MB/s]Downloading:  16%|█▌        | 68.5M/440M [00:03<00:23, 15.8MB/s]Downloading:  16%|█▌        | 70.2M/440M [00:03<00:24, 15.2MB/s]Downloading:  16%|█▋        | 71.9M/440M [00:03<00:24, 14.9MB/s]Downloading:  17%|█▋        | 73.5M/440M [00:03<00:24, 15.3MB/s]Downloading:  17%|█▋        | 75.2M/440M [00:04<00:23, 15.8MB/s]Downloading:  17%|█▋        | 76.9M/440M [00:04<00:29, 12.1MB/s]Downloading:  18%|█▊        | 79.1M/440M [00:04<00:25, 14.0MB/s]Downloading:  18%|█▊        | 81.0M/440M [00:04<00:23, 15.2MB/s]Downloading:  19%|█▉        | 82.7M/440M [00:04<00:24, 14.5MB/s]Downloading:  19%|█▉        | 84.4M/440M [00:04<00:23, 15.0MB/s]Downloading:  20%|█▉        | 86.1M/440M [00:04<00:23, 15.4MB/s]Downloading:  20%|█▉        | 87.7M/440M [00:04<00:22, 15.4MB/s]Downloading:  20%|██        | 89.4M/440M [00:05<00:22, 15.3MB/s]Downloading:  21%|██        | 91.1M/440M [00:05<00:22, 15.8MB/s]Downloading:  21%|██        | 92.9M/440M [00:05<00:21, 16.2MB/s]Downloading:  21%|██▏       | 94.7M/440M [00:05<00:20, 16.7MB/s]Downloading:  22%|██▏       | 96.4M/440M [00:05<00:20, 16.6MB/s]Downloading:  22%|██▏       | 98.1M/440M [00:05<00:27, 12.7MB/s]Downloading:  23%|██▎       | 100M/440M [00:05<00:23, 14.6MB/s] Downloading:  23%|██▎       | 102M/440M [00:05<00:24, 13.7MB/s]Downloading:  24%|██▎       | 104M/440M [00:05<00:24, 13.9MB/s]Downloading:  24%|██▍       | 105M/440M [00:06<00:23, 14.0MB/s]Downloading:  24%|██▍       | 107M/440M [00:06<00:28, 11.6MB/s]Downloading:  25%|██▍       | 108M/440M [00:06<00:26, 12.4MB/s]Downloading:  25%|██▍       | 110M/440M [00:06<00:28, 11.8MB/s]Downloading:  25%|██▌       | 111M/440M [00:06<00:25, 13.0MB/s]Downloading:  26%|██▌       | 113M/440M [00:06<00:25, 12.8MB/s]Downloading:  26%|██▌       | 114M/440M [00:06<00:26, 12.5MB/s]Downloading:  26%|██▌       | 115M/440M [00:06<00:26, 12.0MB/s]Downloading:  26%|██▋       | 117M/440M [00:07<00:28, 11.5MB/s]Downloading:  27%|██▋       | 118M/440M [00:07<00:24, 12.9MB/s]Downloading:  27%|██▋       | 120M/440M [00:07<00:25, 12.5MB/s]Downloading:  28%|██▊       | 121M/440M [00:07<00:24, 12.9MB/s]Downloading:  28%|██▊       | 123M/440M [00:07<00:25, 12.5MB/s]Downloading:  28%|██▊       | 124M/440M [00:07<00:35, 8.79MB/s]Downloading:  28%|██▊       | 125M/440M [00:07<00:32, 9.58MB/s]Downloading:  29%|██▊       | 127M/440M [00:07<00:30, 10.2MB/s]Downloading:  29%|██▉       | 128M/440M [00:08<00:29, 10.8MB/s]Downloading:  29%|██▉       | 129M/440M [00:08<00:28, 10.8MB/s]Downloading:  30%|██▉       | 130M/440M [00:08<00:29, 10.4MB/s]Downloading:  30%|██▉       | 132M/440M [00:08<00:26, 11.7MB/s]Downloading:  30%|███       | 133M/440M [00:08<00:26, 11.5MB/s]Downloading:  31%|███       | 134M/440M [00:08<00:29, 10.5MB/s]Downloading:  31%|███       | 136M/440M [00:08<00:24, 12.4MB/s]Downloading:  31%|███▏      | 138M/440M [00:08<00:22, 13.2MB/s]Downloading:  32%|███▏      | 140M/440M [00:08<00:20, 14.4MB/s]Downloading:  32%|███▏      | 141M/440M [00:09<00:24, 12.4MB/s]Downloading:  32%|███▏      | 143M/440M [00:09<00:23, 12.5MB/s]Downloading:  33%|███▎      | 145M/440M [00:09<00:21, 14.0MB/s]Downloading:  33%|███▎      | 146M/440M [00:09<00:21, 13.7MB/s]Downloading:  34%|███▎      | 148M/440M [00:09<00:20, 14.5MB/s]Downloading:  34%|███▍      | 150M/440M [00:09<00:20, 14.0MB/s]Downloading:  34%|███▍      | 151M/440M [00:09<00:24, 12.0MB/s]Downloading:  35%|███▍      | 152M/440M [00:10<00:28, 10.2MB/s]Downloading:  35%|███▍      | 154M/440M [00:10<00:30, 9.36MB/s]Downloading:  35%|███▌      | 155M/440M [00:10<00:28, 9.96MB/s]Downloading:  36%|███▌      | 157M/440M [00:10<00:25, 11.3MB/s]Downloading:  36%|███▌      | 158M/440M [00:10<00:23, 12.1MB/s]Downloading:  36%|███▌      | 159M/440M [00:10<00:22, 12.3MB/s]Downloading:  37%|███▋      | 161M/440M [00:10<00:21, 13.3MB/s]Downloading:  37%|███▋      | 162M/440M [00:10<00:22, 12.6MB/s]Downloading:  37%|███▋      | 164M/440M [00:10<00:21, 12.9MB/s]Downloading:  38%|███▊      | 165M/440M [00:11<00:19, 13.8MB/s]Downloading:  38%|███▊      | 168M/440M [00:11<00:18, 14.4MB/s]Downloading:  39%|███▊      | 170M/440M [00:11<00:17, 15.4MB/s]Downloading:  39%|███▉      | 172M/440M [00:11<00:16, 16.6MB/s]Downloading:  39%|███▉      | 174M/440M [00:11<00:15, 17.4MB/s]Downloading:  40%|███▉      | 175M/440M [00:11<00:15, 17.3MB/s]Downloading:  40%|████      | 177M/440M [00:11<00:15, 16.6MB/s]Downloading:  41%|████      | 179M/440M [00:11<00:16, 15.6MB/s]Downloading:  41%|████      | 181M/440M [00:11<00:15, 16.7MB/s]Downloading:  42%|████▏     | 184M/440M [00:12<00:13, 18.4MB/s]Downloading:  42%|████▏     | 185M/440M [00:12<00:15, 16.3MB/s]Downloading:  43%|████▎     | 188M/440M [00:12<00:14, 17.8MB/s]Downloading:  43%|████▎     | 190M/440M [00:12<00:14, 17.8MB/s]Downloading:  43%|████▎     | 192M/440M [00:12<00:13, 17.9MB/s]Downloading:  44%|████▍     | 193M/440M [00:12<00:14, 17.3MB/s]Downloading:  44%|████▍     | 195M/440M [00:12<00:13, 17.9MB/s]Downloading:  45%|████▍     | 197M/440M [00:12<00:13, 17.5MB/s]Downloading:  45%|████▌     | 199M/440M [00:12<00:12, 18.8MB/s]Downloading:  46%|████▌     | 201M/440M [00:13<00:13, 18.2MB/s]Downloading:  46%|████▋     | 204M/440M [00:13<00:11, 20.1MB/s]Downloading:  47%|████▋     | 206M/440M [00:13<00:11, 20.6MB/s]Downloading:  47%|████▋     | 208M/440M [00:13<00:12, 19.2MB/s]Downloading:  48%|████▊     | 210M/440M [00:13<00:12, 19.2MB/s]Downloading:  48%|████▊     | 212M/440M [00:13<00:12, 17.9MB/s]Downloading:  49%|████▊     | 214M/440M [00:13<00:12, 18.5MB/s]Downloading:  49%|████▉     | 216M/440M [00:13<00:18, 12.2MB/s]Downloading:  50%|████▉     | 219M/440M [00:14<00:15, 14.7MB/s]Downloading:  50%|█████     | 221M/440M [00:14<00:15, 13.9MB/s]Downloading:  51%|█████     | 223M/440M [00:14<00:16, 13.1MB/s]Downloading:  51%|█████     | 225M/440M [00:14<00:15, 14.1MB/s]Downloading:  51%|█████▏    | 226M/440M [00:14<00:15, 13.4MB/s]Downloading:  52%|█████▏    | 228M/440M [00:14<00:14, 14.4MB/s]Downloading:  52%|█████▏    | 230M/440M [00:14<00:13, 15.9MB/s]Downloading:  53%|█████▎    | 232M/440M [00:14<00:13, 14.9MB/s]Downloading:  53%|█████▎    | 234M/440M [00:15<00:14, 14.7MB/s]Downloading:  53%|█████▎    | 235M/440M [00:15<00:14, 14.0MB/s]Downloading:  54%|█████▍    | 237M/440M [00:15<00:13, 15.3MB/s]Downloading:  54%|█████▍    | 239M/440M [00:15<00:14, 13.8MB/s]Downloading:  55%|█████▍    | 241M/440M [00:15<00:13, 14.8MB/s]Downloading:  55%|█████▍    | 242M/440M [00:15<00:13, 14.7MB/s]Downloading:  55%|█████▌    | 244M/440M [00:15<00:12, 15.8MB/s]Downloading:  56%|█████▌    | 246M/440M [00:15<00:11, 16.3MB/s]Downloading:  56%|█████▋    | 248M/440M [00:15<00:11, 17.3MB/s]Downloading:  57%|█████▋    | 250M/440M [00:16<00:10, 18.0MB/s]Downloading:  57%|█████▋    | 252M/440M [00:16<00:09, 19.2MB/s]Downloading:  58%|█████▊    | 254M/440M [00:16<00:09, 19.3MB/s]Downloading:  58%|█████▊    | 257M/440M [00:16<00:08, 20.7MB/s]Downloading:  59%|█████▉    | 259M/440M [00:16<00:10, 17.2MB/s]Downloading:  59%|█████▉    | 261M/440M [00:16<00:12, 14.7MB/s]Downloading:  60%|█████▉    | 263M/440M [00:16<00:10, 16.4MB/s]Downloading:  60%|██████    | 265M/440M [00:16<00:12, 14.2MB/s]Downloading:  61%|██████    | 267M/440M [00:17<00:11, 15.7MB/s]Downloading:  61%|██████    | 269M/440M [00:17<00:11, 15.1MB/s]Downloading:  61%|██████▏   | 270M/440M [00:17<00:13, 13.0MB/s]Downloading:  62%|██████▏   | 273M/440M [00:17<00:11, 14.8MB/s]Downloading:  62%|██████▏   | 274M/440M [00:17<00:11, 14.6MB/s]Downloading:  63%|██████▎   | 276M/440M [00:17<00:10, 15.5MB/s]Downloading:  63%|██████▎   | 278M/440M [00:17<00:10, 14.8MB/s]Downloading:  63%|██████▎   | 279M/440M [00:17<00:10, 15.7MB/s]Downloading:  64%|██████▍   | 281M/440M [00:18<00:10, 14.5MB/s]Downloading:  64%|██████▍   | 283M/440M [00:18<00:11, 14.2MB/s]Downloading:  65%|██████▍   | 284M/440M [00:18<00:10, 14.4MB/s]Downloading:  65%|██████▍   | 286M/440M [00:18<00:10, 14.1MB/s]Downloading:  65%|██████▌   | 288M/440M [00:18<00:09, 15.7MB/s]Downloading:  66%|██████▌   | 290M/440M [00:18<00:09, 15.8MB/s]Downloading:  66%|██████▌   | 291M/440M [00:18<00:09, 15.5MB/s]Downloading:  67%|██████▋   | 293M/440M [00:18<00:09, 16.2MB/s]Downloading:  67%|██████▋   | 295M/440M [00:18<00:08, 17.1MB/s]Downloading:  67%|██████▋   | 297M/440M [00:18<00:07, 18.0MB/s]Downloading:  68%|██████▊   | 299M/440M [00:19<00:07, 18.1MB/s]Downloading:  68%|██████▊   | 301M/440M [00:19<00:09, 15.5MB/s]Downloading:  69%|██████▊   | 302M/440M [00:19<00:11, 11.6MB/s]Downloading:  69%|██████▉   | 304M/440M [00:19<00:12, 11.0MB/s]Downloading:  69%|██████▉   | 306M/440M [00:19<00:10, 12.4MB/s]Downloading:  70%|██████▉   | 307M/440M [00:19<00:11, 11.7MB/s]Downloading:  70%|███████   | 308M/440M [00:19<00:10, 12.1MB/s]Downloading:  70%|███████   | 310M/440M [00:20<00:10, 12.4MB/s]Downloading:  71%|███████   | 312M/440M [00:20<00:09, 13.8MB/s]Downloading:  71%|███████   | 314M/440M [00:20<00:08, 15.3MB/s]Downloading:  72%|███████▏  | 315M/440M [00:20<00:08, 14.7MB/s]Downloading:  72%|███████▏  | 317M/440M [00:20<00:07, 16.3MB/s]Downloading:  73%|███████▎  | 319M/440M [00:20<00:07, 17.3MB/s]Downloading:  73%|███████▎  | 322M/440M [00:20<00:06, 18.5MB/s]Downloading:  73%|███████▎  | 324M/440M [00:20<00:06, 18.6MB/s]Downloading:  74%|███████▍  | 326M/440M [00:20<00:06, 16.5MB/s]Downloading:  74%|███████▍  | 327M/440M [00:21<00:06, 16.8MB/s]Downloading:  75%|███████▍  | 329M/440M [00:21<00:07, 15.7MB/s]Downloading:  75%|███████▌  | 331M/440M [00:21<00:08, 13.7MB/s]Downloading:  75%|███████▌  | 332M/440M [00:21<00:07, 14.6MB/s]Downloading:  76%|███████▌  | 334M/440M [00:21<00:07, 14.7MB/s]Downloading:  76%|███████▌  | 336M/440M [00:21<00:06, 15.7MB/s]Downloading:  77%|███████▋  | 337M/440M [00:21<00:06, 15.9MB/s]Downloading:  77%|███████▋  | 339M/440M [00:21<00:06, 16.6MB/s]Downloading:  77%|███████▋  | 341M/440M [00:21<00:05, 16.9MB/s]Downloading:  78%|███████▊  | 343M/440M [00:22<00:05, 16.5MB/s]Downloading:  78%|███████▊  | 345M/440M [00:22<00:06, 15.5MB/s]Downloading:  79%|███████▊  | 346M/440M [00:22<00:06, 13.5MB/s]Downloading:  79%|███████▉  | 349M/440M [00:22<00:05, 16.3MB/s]Downloading:  80%|███████▉  | 351M/440M [00:22<00:06, 14.1MB/s]Downloading:  80%|████████  | 353M/440M [00:22<00:05, 15.1MB/s]Downloading:  81%|████████  | 355M/440M [00:22<00:05, 14.7MB/s]Downloading:  81%|████████  | 357M/440M [00:22<00:05, 16.3MB/s]Downloading:  82%|████████▏ | 359M/440M [00:23<00:04, 18.0MB/s]Downloading:  82%|████████▏ | 362M/440M [00:23<00:04, 19.1MB/s]Downloading:  83%|████████▎ | 364M/440M [00:23<00:04, 16.4MB/s]Downloading:  83%|████████▎ | 366M/440M [00:23<00:04, 17.4MB/s]Downloading:  83%|████████▎ | 368M/440M [00:23<00:04, 17.4MB/s]Downloading:  84%|████████▍ | 369M/440M [00:23<00:04, 17.2MB/s]Downloading:  84%|████████▍ | 371M/440M [00:23<00:03, 17.6MB/s]Downloading:  85%|████████▍ | 373M/440M [00:23<00:04, 15.3MB/s]Downloading:  85%|████████▌ | 375M/440M [00:23<00:03, 16.7MB/s]Downloading:  86%|████████▌ | 377M/440M [00:24<00:03, 18.1MB/s]Downloading:  86%|████████▌ | 379M/440M [00:24<00:03, 16.1MB/s]Downloading:  87%|████████▋ | 381M/440M [00:24<00:03, 16.2MB/s]Downloading:  87%|████████▋ | 383M/440M [00:24<00:04, 13.5MB/s]Downloading:  87%|████████▋ | 384M/440M [00:24<00:04, 11.3MB/s]Downloading:  88%|████████▊ | 386M/440M [00:24<00:04, 11.2MB/s]Downloading:  88%|████████▊ | 387M/440M [00:24<00:04, 11.6MB/s]Downloading:  88%|████████▊ | 388M/440M [00:24<00:04, 11.1MB/s]Downloading:  88%|████████▊ | 390M/440M [00:25<00:04, 12.1MB/s]Downloading:  89%|████████▊ | 391M/440M [00:25<00:04, 12.3MB/s]Downloading:  89%|████████▉ | 392M/440M [00:25<00:03, 13.2MB/s]Downloading:  89%|████████▉ | 394M/440M [00:25<00:03, 13.8MB/s]Downloading:  90%|████████▉ | 395M/440M [00:25<00:03, 12.2MB/s]Downloading:  90%|█████████ | 398M/440M [00:25<00:02, 14.8MB/s]Downloading:  91%|█████████ | 400M/440M [00:25<00:02, 14.8MB/s]Downloading:  91%|█████████▏| 402M/440M [00:25<00:02, 16.0MB/s]Downloading:  92%|█████████▏| 404M/440M [00:25<00:02, 15.8MB/s]Downloading:  92%|█████████▏| 406M/440M [00:26<00:02, 15.5MB/s]Downloading:  93%|█████████▎| 407M/440M [00:26<00:02, 16.1MB/s]Downloading:  93%|█████████▎| 410M/440M [00:26<00:01, 18.3MB/s]Downloading:  94%|█████████▎| 412M/440M [00:26<00:01, 17.9MB/s]Downloading:  94%|█████████▍| 414M/440M [00:26<00:01, 14.7MB/s]Downloading:  94%|█████████▍| 416M/440M [00:26<00:01, 14.8MB/s]Downloading:  95%|█████████▍| 418M/440M [00:26<00:01, 15.7MB/s]Downloading:  95%|█████████▌| 419M/440M [00:26<00:01, 14.5MB/s]Downloading:  96%|█████████▌| 421M/440M [00:27<00:01, 14.9MB/s]Downloading:  96%|█████████▌| 423M/440M [00:27<00:01, 15.1MB/s]Downloading:  96%|█████████▋| 424M/440M [00:27<00:01, 12.9MB/s]Downloading:  97%|█████████▋| 426M/440M [00:27<00:01, 12.2MB/s]Downloading:  97%|█████████▋| 427M/440M [00:27<00:01, 12.8MB/s]Downloading:  97%|█████████▋| 428M/440M [00:27<00:01, 12.0MB/s]Downloading:  98%|█████████▊| 430M/440M [00:27<00:00, 13.4MB/s]Downloading:  98%|█████████▊| 432M/440M [00:27<00:00, 13.9MB/s]Downloading:  98%|█████████▊| 433M/440M [00:27<00:00, 14.8MB/s]Downloading:  99%|█████████▉| 435M/440M [00:28<00:00, 15.9MB/s]Downloading:  99%|█████████▉| 437M/440M [00:28<00:00, 16.7MB/s]Downloading: 100%|█████████▉| 439M/440M [00:28<00:00, 16.7MB/s]Downloading: 100%|██████████| 440M/440M [00:28<00:00, 15.5MB/s]
['trump', 'asked', 'du', '##ter', '##te', 'if', 'philippines', 'has', 'death', 'limit', ',', 'philippines', 'ambassador', 'says']
[8398, 2356, 4241, 3334, 2618, 2065, 5137, 2038, 2331, 5787, 1010, 5137, 6059, 2758]
Training model.
| Epoch: 01 | Train Loss: 0.49 | Train MSE: 0.49 | Train RMSE: 0.70 |
| Epoch: 02 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.60 |
| Epoch: 03 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.60 |
| Epoch: 04 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 05 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 06 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 07 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.59 |
| Epoch: 08 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.59 |
| Epoch: 09 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 10 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 11 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 12 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 13 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 14 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 15 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 16 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 17 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 18 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 19 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.56 |
| Epoch: 20 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| MSE: 0.31 | RMSE: 0.56 |
Current Hyperparameters:
Batch Size: 64, Hidden Size: 64, Total Layers: 3, Dropout: 0.2, Learning Rate: 0.01
MSE: 0.31465229392051697, RMSE: 0.5609387755393982
Found better hyperparameters...

Training model.
| Epoch: 01 | Train Loss: 0.37 | Train MSE: 0.37 | Train RMSE: 0.61 |
| Epoch: 02 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.59 |
| Epoch: 03 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 04 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 05 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 06 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 07 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.56 |
| Epoch: 08 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.56 |
| Epoch: 09 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| Epoch: 10 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| Epoch: 11 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55 |
| Epoch: 12 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.54 |
| Epoch: 13 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.54 |
| Epoch: 14 | Train Loss: 0.28 | Train MSE: 0.28 | Train RMSE: 0.53 |
| Epoch: 15 | Train Loss: 0.28 | Train MSE: 0.28 | Train RMSE: 0.53 |
| Epoch: 16 | Train Loss: 0.27 | Train MSE: 0.27 | Train RMSE: 0.52 |
| Epoch: 17 | Train Loss: 0.27 | Train MSE: 0.27 | Train RMSE: 0.52 |
| Epoch: 18 | Train Loss: 0.26 | Train MSE: 0.26 | Train RMSE: 0.51 |
| Epoch: 19 | Train Loss: 0.26 | Train MSE: 0.26 | Train RMSE: 0.51 |
| Epoch: 20 | Train Loss: 0.26 | Train MSE: 0.26 | Train RMSE: 0.51 |
| MSE: 0.36 | RMSE: 0.60 |
Current Hyperparameters:
Batch Size: 64, Hidden Size: 64, Total Layers: 3, Dropout: 0.2, Learning Rate: 0.001
MSE: 0.3592483699321747, RMSE: 0.5993733406066895

Training model.
| Epoch: 01 | Train Loss: 0.48 | Train MSE: 0.48 | Train RMSE: 0.69 |
| Epoch: 02 | Train Loss: 0.37 | Train MSE: 0.37 | Train RMSE: 0.60 |
| Epoch: 03 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 04 | Train Loss: 0.37 | Train MSE: 0.37 | Train RMSE: 0.61 |
| Epoch: 05 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 06 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 07 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 08 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 09 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 10 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 11 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 12 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |
| Epoch: 13 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 14 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |
| Epoch: 15 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |
| Epoch: 16 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 17 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 18 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 19 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 20 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| MSE: 0.33 | RMSE: 0.57 |
Current Hyperparameters:
Batch Size: 64, Hidden Size: 64, Total Layers: 3, Dropout: 0.4, Learning Rate: 0.01
MSE: 0.3305451571941376, RMSE: 0.5749305486679077

Training model.
| Epoch: 01 | Train Loss: 0.39 | Train MSE: 0.39 | Train RMSE: 0.62 |
| Epoch: 02 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 03 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 04 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 05 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 06 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |
| Epoch: 07 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 08 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 09 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 10 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 11 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| Epoch: 12 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| Epoch: 13 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.55 |
| Epoch: 14 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55 |
| Epoch: 15 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.54 |
| Epoch: 16 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.54 |
| Epoch: 17 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.54 |
| Epoch: 18 | Train Loss: 0.28 | Train MSE: 0.28 | Train RMSE: 0.53 |
| Epoch: 19 | Train Loss: 0.28 | Train MSE: 0.28 | Train RMSE: 0.53 |
| Epoch: 20 | Train Loss: 0.27 | Train MSE: 0.27 | Train RMSE: 0.52 |
| MSE: 0.35 | RMSE: 0.59 |
Current Hyperparameters:
Batch Size: 64, Hidden Size: 64, Total Layers: 3, Dropout: 0.4, Learning Rate: 0.001
MSE: 0.3494647741317749, RMSE: 0.591155469417572

Training model.
| Epoch: 01 | Train Loss: 0.44 | Train MSE: 0.44 | Train RMSE: 0.66 |
| Epoch: 02 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 03 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 04 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 05 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 06 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 07 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 08 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 09 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 10 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 11 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 12 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 13 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 14 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.59 |
| Epoch: 15 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.59 |
| Epoch: 16 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.59 |
| Epoch: 17 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.59 |
| Epoch: 18 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.59 |
| Epoch: 19 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.59 |
| Epoch: 20 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.59 |
| MSE: 0.34 | RMSE: 0.58 |
Current Hyperparameters:
Batch Size: 64, Hidden Size: 64, Total Layers: 5, Dropout: 0.2, Learning Rate: 0.01
MSE: 0.34051409363746643, RMSE: 0.5835358500480652

Training model.
| Epoch: 01 | Train Loss: 0.37 | Train MSE: 0.37 | Train RMSE: 0.61 |
| Epoch: 02 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 03 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 04 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |
| Epoch: 05 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |
| Epoch: 06 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |
| Epoch: 07 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 08 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.56 |
| Epoch: 09 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| Epoch: 10 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.55 |
| Epoch: 11 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55 |
| Epoch: 12 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55 |
| Epoch: 13 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.54 |
| Epoch: 14 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.54 |
| Epoch: 15 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.53 |
| Epoch: 16 | Train Loss: 0.28 | Train MSE: 0.28 | Train RMSE: 0.53 |
| Epoch: 17 | Train Loss: 0.27 | Train MSE: 0.27 | Train RMSE: 0.52 |
| Epoch: 18 | Train Loss: 0.27 | Train MSE: 0.27 | Train RMSE: 0.52 |
| Epoch: 19 | Train Loss: 0.26 | Train MSE: 0.26 | Train RMSE: 0.51 |
| Epoch: 20 | Train Loss: 0.25 | Train MSE: 0.25 | Train RMSE: 0.50 |
| MSE: 0.34 | RMSE: 0.58 |
Current Hyperparameters:
Batch Size: 64, Hidden Size: 64, Total Layers: 5, Dropout: 0.2, Learning Rate: 0.001
MSE: 0.33816197514533997, RMSE: 0.5815169811248779

Training model.
| Epoch: 01 | Train Loss: 0.44 | Train MSE: 0.44 | Train RMSE: 0.66 |
| Epoch: 02 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 03 | Train Loss: 0.37 | Train MSE: 0.37 | Train RMSE: 0.61 |
| Epoch: 04 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 05 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 06 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 07 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 08 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 09 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 10 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 11 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 12 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 13 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.59 |
| Epoch: 14 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.59 |
| Epoch: 15 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.59 |
| Epoch: 16 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.59 |
| Epoch: 17 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.59 |
| Epoch: 18 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 19 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.59 |
| Epoch: 20 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.59 |
| MSE: 0.34 | RMSE: 0.58 |
Current Hyperparameters:
Batch Size: 64, Hidden Size: 64, Total Layers: 5, Dropout: 0.4, Learning Rate: 0.01
MSE: 0.33815136551856995, RMSE: 0.5815078616142273

Training model.
| Epoch: 01 | Train Loss: 0.39 | Train MSE: 0.39 | Train RMSE: 0.62 |
| Epoch: 02 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 03 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 04 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.59 |
| Epoch: 05 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 06 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |
| Epoch: 07 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 08 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 09 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 10 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 11 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 12 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| Epoch: 13 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| Epoch: 14 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55 |
| Epoch: 15 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55 |
| Epoch: 16 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.54 |
| Epoch: 17 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.54 |
| Epoch: 18 | Train Loss: 0.28 | Train MSE: 0.28 | Train RMSE: 0.53 |
| Epoch: 19 | Train Loss: 0.27 | Train MSE: 0.27 | Train RMSE: 0.52 |
| Epoch: 20 | Train Loss: 0.27 | Train MSE: 0.27 | Train RMSE: 0.52 |
| MSE: 0.36 | RMSE: 0.60 |
Current Hyperparameters:
Batch Size: 64, Hidden Size: 64, Total Layers: 5, Dropout: 0.4, Learning Rate: 0.001
MSE: 0.357805997133255, RMSE: 0.5981688499450684

Training model.
| Epoch: 01 | Train Loss: 1.28 | Train MSE: 1.28 | Train RMSE: 1.13 |
| Epoch: 02 | Train Loss: 0.42 | Train MSE: 0.42 | Train RMSE: 0.65 |
| Epoch: 03 | Train Loss: 0.38 | Train MSE: 0.38 | Train RMSE: 0.62 |
| Epoch: 04 | Train Loss: 0.38 | Train MSE: 0.38 | Train RMSE: 0.61 |
| Epoch: 05 | Train Loss: 0.38 | Train MSE: 0.38 | Train RMSE: 0.61 |
| Epoch: 06 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 07 | Train Loss: 0.38 | Train MSE: 0.38 | Train RMSE: 0.62 |
| Epoch: 08 | Train Loss: 0.38 | Train MSE: 0.38 | Train RMSE: 0.62 |
| Epoch: 09 | Train Loss: 0.39 | Train MSE: 0.39 | Train RMSE: 0.62 |
| Epoch: 10 | Train Loss: 0.39 | Train MSE: 0.39 | Train RMSE: 0.62 |
| Epoch: 11 | Train Loss: 0.44 | Train MSE: 0.44 | Train RMSE: 0.66 |
| Epoch: 12 | Train Loss: 0.40 | Train MSE: 0.40 | Train RMSE: 0.63 |
| Epoch: 13 | Train Loss: 0.45 | Train MSE: 0.45 | Train RMSE: 0.67 |
| Epoch: 14 | Train Loss: 0.39 | Train MSE: 0.39 | Train RMSE: 0.63 |
| Epoch: 15 | Train Loss: 0.39 | Train MSE: 0.39 | Train RMSE: 0.62 |
| Epoch: 16 | Train Loss: 0.39 | Train MSE: 0.39 | Train RMSE: 0.62 |
| Epoch: 17 | Train Loss: 0.42 | Train MSE: 0.42 | Train RMSE: 0.65 |
| Epoch: 18 | Train Loss: 0.38 | Train MSE: 0.38 | Train RMSE: 0.61 |
| Epoch: 19 | Train Loss: 0.40 | Train MSE: 0.40 | Train RMSE: 0.63 |
| Epoch: 20 | Train Loss: 0.38 | Train MSE: 0.38 | Train RMSE: 0.61 |
| MSE: 0.41 | RMSE: 0.64 |
Current Hyperparameters:
Batch Size: 64, Hidden Size: 256, Total Layers: 3, Dropout: 0.2, Learning Rate: 0.01
MSE: 0.40538451075553894, RMSE: 0.6366981267929077

Training model.
| Epoch: 01 | Train Loss: 0.41 | Train MSE: 0.41 | Train RMSE: 0.64 |
| Epoch: 02 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 03 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 04 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |
| Epoch: 05 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |
| Epoch: 06 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 07 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.56 |
| Epoch: 08 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| Epoch: 09 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| Epoch: 10 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55 |
| Epoch: 11 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.54 |
| Epoch: 12 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.54 |
| Epoch: 13 | Train Loss: 0.28 | Train MSE: 0.28 | Train RMSE: 0.53 |
| Epoch: 14 | Train Loss: 0.27 | Train MSE: 0.27 | Train RMSE: 0.52 |
| Epoch: 15 | Train Loss: 0.26 | Train MSE: 0.26 | Train RMSE: 0.51 |
| Epoch: 16 | Train Loss: 0.26 | Train MSE: 0.26 | Train RMSE: 0.51 |
| Epoch: 17 | Train Loss: 0.25 | Train MSE: 0.25 | Train RMSE: 0.50 |
| Epoch: 18 | Train Loss: 0.24 | Train MSE: 0.24 | Train RMSE: 0.49 |
| Epoch: 19 | Train Loss: 0.24 | Train MSE: 0.24 | Train RMSE: 0.49 |
| Epoch: 20 | Train Loss: 0.23 | Train MSE: 0.23 | Train RMSE: 0.48 |
| MSE: 0.34 | RMSE: 0.58 |
Current Hyperparameters:
Batch Size: 64, Hidden Size: 256, Total Layers: 3, Dropout: 0.2, Learning Rate: 0.001
MSE: 0.3404056131839752, RMSE: 0.5834428668022156

Training model.
| Epoch: 01 | Train Loss: 1.50 | Train MSE: 1.50 | Train RMSE: 1.22 |
| Epoch: 02 | Train Loss: 0.39 | Train MSE: 0.39 | Train RMSE: 0.63 |
| Epoch: 03 | Train Loss: 0.39 | Train MSE: 0.39 | Train RMSE: 0.63 |
| Epoch: 04 | Train Loss: 0.39 | Train MSE: 0.39 | Train RMSE: 0.62 |
| Epoch: 05 | Train Loss: 0.38 | Train MSE: 0.38 | Train RMSE: 0.62 |
| Epoch: 06 | Train Loss: 0.42 | Train MSE: 0.42 | Train RMSE: 0.65 |
| Epoch: 07 | Train Loss: 0.39 | Train MSE: 0.39 | Train RMSE: 0.63 |
| Epoch: 08 | Train Loss: 0.43 | Train MSE: 0.43 | Train RMSE: 0.66 |
| Epoch: 09 | Train Loss: 0.40 | Train MSE: 0.40 | Train RMSE: 0.64 |
| Epoch: 10 | Train Loss: 0.39 | Train MSE: 0.39 | Train RMSE: 0.63 |
| Epoch: 11 | Train Loss: 0.41 | Train MSE: 0.41 | Train RMSE: 0.64 |
| Epoch: 12 | Train Loss: 0.43 | Train MSE: 0.43 | Train RMSE: 0.65 |
| Epoch: 13 | Train Loss: 0.41 | Train MSE: 0.41 | Train RMSE: 0.64 |
| Epoch: 14 | Train Loss: 0.40 | Train MSE: 0.40 | Train RMSE: 0.64 |
| Epoch: 15 | Train Loss: 0.41 | Train MSE: 0.41 | Train RMSE: 0.64 |
| Epoch: 16 | Train Loss: 0.42 | Train MSE: 0.42 | Train RMSE: 0.65 |
| Epoch: 17 | Train Loss: 0.40 | Train MSE: 0.40 | Train RMSE: 0.63 |
| Epoch: 18 | Train Loss: 0.48 | Train MSE: 0.48 | Train RMSE: 0.69 |
| Epoch: 19 | Train Loss: 0.44 | Train MSE: 0.44 | Train RMSE: 0.66 |
| Epoch: 20 | Train Loss: 0.41 | Train MSE: 0.41 | Train RMSE: 0.64 |
| MSE: 0.38 | RMSE: 0.62 |
Current Hyperparameters:
Batch Size: 64, Hidden Size: 256, Total Layers: 3, Dropout: 0.4, Learning Rate: 0.01
MSE: 0.38256537914276123, RMSE: 0.6185187101364136

Training model.
| Epoch: 01 | Train Loss: 0.42 | Train MSE: 0.42 | Train RMSE: 0.65 |
| Epoch: 02 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 03 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 04 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 05 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |
| Epoch: 06 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 07 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.56 |
| Epoch: 08 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.56 |
| Epoch: 09 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 10 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.56 |
| Epoch: 11 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| Epoch: 12 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55 |
| Epoch: 13 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.54 |
| Epoch: 14 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.54 |
| Epoch: 15 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.54 |
| Epoch: 16 | Train Loss: 0.28 | Train MSE: 0.28 | Train RMSE: 0.53 |
| Epoch: 17 | Train Loss: 0.28 | Train MSE: 0.28 | Train RMSE: 0.53 |
| Epoch: 18 | Train Loss: 0.27 | Train MSE: 0.27 | Train RMSE: 0.52 |
| Epoch: 19 | Train Loss: 0.26 | Train MSE: 0.26 | Train RMSE: 0.51 |
| Epoch: 20 | Train Loss: 0.25 | Train MSE: 0.25 | Train RMSE: 0.50 |
| MSE: 0.34 | RMSE: 0.58 |
Current Hyperparameters:
Batch Size: 64, Hidden Size: 256, Total Layers: 3, Dropout: 0.4, Learning Rate: 0.001
MSE: 0.3404937982559204, RMSE: 0.5835184454917908

Training model.
| Epoch: 01 | Train Loss: 0.93 | Train MSE: 0.93 | Train RMSE: 0.96 |
| Epoch: 02 | Train Loss: 0.42 | Train MSE: 0.42 | Train RMSE: 0.65 |
| Epoch: 03 | Train Loss: 0.38 | Train MSE: 0.38 | Train RMSE: 0.62 |
| Epoch: 04 | Train Loss: 0.44 | Train MSE: 0.44 | Train RMSE: 0.66 |
| Epoch: 05 | Train Loss: 0.40 | Train MSE: 0.40 | Train RMSE: 0.64 |
| Epoch: 06 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 07 | Train Loss: 0.38 | Train MSE: 0.38 | Train RMSE: 0.61 |
| Epoch: 08 | Train Loss: 0.41 | Train MSE: 0.41 | Train RMSE: 0.64 |
| Epoch: 09 | Train Loss: 0.43 | Train MSE: 0.43 | Train RMSE: 0.65 |
| Epoch: 10 | Train Loss: 0.41 | Train MSE: 0.41 | Train RMSE: 0.64 |
| Epoch: 11 | Train Loss: 0.40 | Train MSE: 0.40 | Train RMSE: 0.63 |
| Epoch: 12 | Train Loss: 0.44 | Train MSE: 0.44 | Train RMSE: 0.66 |
| Epoch: 13 | Train Loss: 0.40 | Train MSE: 0.40 | Train RMSE: 0.63 |
| Epoch: 14 | Train Loss: 0.39 | Train MSE: 0.39 | Train RMSE: 0.62 |
| Epoch: 15 | Train Loss: 0.41 | Train MSE: 0.41 | Train RMSE: 0.64 |
| Epoch: 16 | Train Loss: 0.40 | Train MSE: 0.40 | Train RMSE: 0.63 |
| Epoch: 17 | Train Loss: 0.42 | Train MSE: 0.42 | Train RMSE: 0.65 |
| Epoch: 18 | Train Loss: 0.41 | Train MSE: 0.41 | Train RMSE: 0.64 |
| Epoch: 19 | Train Loss: 0.42 | Train MSE: 0.42 | Train RMSE: 0.65 |
| Epoch: 20 | Train Loss: 0.45 | Train MSE: 0.45 | Train RMSE: 0.67 |
| MSE: 0.50 | RMSE: 0.71 |
Current Hyperparameters:
Batch Size: 64, Hidden Size: 256, Total Layers: 5, Dropout: 0.2, Learning Rate: 0.01
MSE: 0.5048069953918457, RMSE: 0.7104977369308472

Training model.
| Epoch: 01 | Train Loss: 0.40 | Train MSE: 0.40 | Train RMSE: 0.63 |
| Epoch: 02 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 03 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.59 |
| Epoch: 04 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 05 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 06 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 07 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 08 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| Epoch: 09 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| Epoch: 10 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55 |
| Epoch: 11 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.54 |
| Epoch: 12 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.54 |
| Epoch: 13 | Train Loss: 0.28 | Train MSE: 0.28 | Train RMSE: 0.53 |
| Epoch: 14 | Train Loss: 0.28 | Train MSE: 0.28 | Train RMSE: 0.53 |
| Epoch: 15 | Train Loss: 0.27 | Train MSE: 0.27 | Train RMSE: 0.52 |
| Epoch: 16 | Train Loss: 0.26 | Train MSE: 0.26 | Train RMSE: 0.51 |
| Epoch: 17 | Train Loss: 0.25 | Train MSE: 0.25 | Train RMSE: 0.50 |
| Epoch: 18 | Train Loss: 0.25 | Train MSE: 0.25 | Train RMSE: 0.50 |
| Epoch: 19 | Train Loss: 0.24 | Train MSE: 0.24 | Train RMSE: 0.49 |
| Epoch: 20 | Train Loss: 0.24 | Train MSE: 0.24 | Train RMSE: 0.49 |
| MSE: 0.34 | RMSE: 0.59 |
Current Hyperparameters:
Batch Size: 64, Hidden Size: 256, Total Layers: 5, Dropout: 0.2, Learning Rate: 0.001
MSE: 0.3446159362792969, RMSE: 0.5870400071144104

Training model.
| Epoch: 01 | Train Loss: 1.10 | Train MSE: 1.10 | Train RMSE: 1.05 |
| Epoch: 02 | Train Loss: 0.41 | Train MSE: 0.41 | Train RMSE: 0.64 |
| Epoch: 03 | Train Loss: 0.39 | Train MSE: 0.39 | Train RMSE: 0.63 |
| Epoch: 04 | Train Loss: 0.40 | Train MSE: 0.40 | Train RMSE: 0.63 |
| Epoch: 05 | Train Loss: 0.40 | Train MSE: 0.40 | Train RMSE: 0.63 |
| Epoch: 06 | Train Loss: 0.42 | Train MSE: 0.42 | Train RMSE: 0.65 |
| Epoch: 07 | Train Loss: 0.43 | Train MSE: 0.43 | Train RMSE: 0.66 |
| Epoch: 08 | Train Loss: 0.40 | Train MSE: 0.40 | Train RMSE: 0.63 |
| Epoch: 09 | Train Loss: 0.46 | Train MSE: 0.46 | Train RMSE: 0.68 |
| Epoch: 10 | Train Loss: 0.41 | Train MSE: 0.41 | Train RMSE: 0.64 |
| Epoch: 11 | Train Loss: 0.42 | Train MSE: 0.42 | Train RMSE: 0.65 |
| Epoch: 12 | Train Loss: 0.47 | Train MSE: 0.47 | Train RMSE: 0.69 |
| Epoch: 13 | Train Loss: 0.41 | Train MSE: 0.41 | Train RMSE: 0.64 |
| Epoch: 14 | Train Loss: 0.41 | Train MSE: 0.41 | Train RMSE: 0.64 |
| Epoch: 15 | Train Loss: 0.43 | Train MSE: 0.43 | Train RMSE: 0.65 |
| Epoch: 16 | Train Loss: 0.48 | Train MSE: 0.48 | Train RMSE: 0.69 |
| Epoch: 17 | Train Loss: 0.42 | Train MSE: 0.42 | Train RMSE: 0.65 |
| Epoch: 18 | Train Loss: 0.47 | Train MSE: 0.47 | Train RMSE: 0.68 |
| Epoch: 19 | Train Loss: 0.42 | Train MSE: 0.42 | Train RMSE: 0.65 |
| Epoch: 20 | Train Loss: 0.49 | Train MSE: 0.49 | Train RMSE: 0.70 |
| MSE: 0.54 | RMSE: 0.74 |
Current Hyperparameters:
Batch Size: 64, Hidden Size: 256, Total Layers: 5, Dropout: 0.4, Learning Rate: 0.01
MSE: 0.54425048828125, RMSE: 0.7377333641052246

Training model.
| Epoch: 01 | Train Loss: 0.41 | Train MSE: 0.41 | Train RMSE: 0.64 |
| Epoch: 02 | Train Loss: 0.37 | Train MSE: 0.37 | Train RMSE: 0.60 |
| Epoch: 03 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 04 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.59 |
| Epoch: 05 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 06 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 07 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 08 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 09 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 10 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.56 |
| Epoch: 11 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| Epoch: 12 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| Epoch: 13 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.55 |
| Epoch: 14 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55 |
| Epoch: 15 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.54 |
| Epoch: 16 | Train Loss: 0.28 | Train MSE: 0.28 | Train RMSE: 0.53 |
| Epoch: 17 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.54 |
| Epoch: 18 | Train Loss: 0.27 | Train MSE: 0.27 | Train RMSE: 0.52 |
| Epoch: 19 | Train Loss: 0.27 | Train MSE: 0.27 | Train RMSE: 0.52 |
| Epoch: 20 | Train Loss: 0.26 | Train MSE: 0.26 | Train RMSE: 0.51 |
| MSE: 0.34 | RMSE: 0.58 |
Current Hyperparameters:
Batch Size: 64, Hidden Size: 256, Total Layers: 5, Dropout: 0.4, Learning Rate: 0.001
MSE: 0.34040653705596924, RMSE: 0.5834437012672424

Training model.
| Epoch: 01 | Train Loss: 0.87 | Train MSE: 0.87 | Train RMSE: 0.93 |
| Epoch: 02 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 03 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 04 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 05 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 06 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 07 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 08 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 09 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 10 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 11 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 12 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 13 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.59 |
| Epoch: 14 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |
| Epoch: 15 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 16 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 17 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 18 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 19 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 20 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| MSE: 0.34 | RMSE: 0.58 |
Current Hyperparameters:
Batch Size: 256, Hidden Size: 64, Total Layers: 3, Dropout: 0.2, Learning Rate: 0.01
MSE: 0.3355965316295624, RMSE: 0.579306960105896

Training model.
| Epoch: 01 | Train Loss: 0.41 | Train MSE: 0.41 | Train RMSE: 0.64 |
| Epoch: 02 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 03 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.59 |
| Epoch: 04 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 05 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 06 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |
| Epoch: 07 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 08 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 09 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 10 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 11 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 12 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 13 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| Epoch: 14 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| Epoch: 15 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55 |
| Epoch: 16 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55 |
| Epoch: 17 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55 |
| Epoch: 18 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55 |
| Epoch: 19 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.54 |
| Epoch: 20 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.54 |
| MSE: 0.33 | RMSE: 0.58 |
Current Hyperparameters:
Batch Size: 256, Hidden Size: 64, Total Layers: 3, Dropout: 0.2, Learning Rate: 0.001
MSE: 0.33188387751579285, RMSE: 0.5760936141014099

Training model.
| Epoch: 01 | Train Loss: 0.72 | Train MSE: 0.72 | Train RMSE: 0.85 |
| Epoch: 02 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 03 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 04 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.60 |
| Epoch: 05 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 06 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 07 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 08 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 09 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 10 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.59 |
| Epoch: 11 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 12 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 13 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 14 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |
| Epoch: 15 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |
| Epoch: 16 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 17 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 18 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 19 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 20 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| MSE: 0.34 | RMSE: 0.59 |
Current Hyperparameters:
Batch Size: 256, Hidden Size: 64, Total Layers: 3, Dropout: 0.4, Learning Rate: 0.01
MSE: 0.3427366316318512, RMSE: 0.5854371190071106

Training model.
| Epoch: 01 | Train Loss: 0.42 | Train MSE: 0.42 | Train RMSE: 0.65 |
| Epoch: 02 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 03 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 04 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.59 |
| Epoch: 05 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.59 |
| Epoch: 06 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 07 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 08 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |
| Epoch: 09 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |
| Epoch: 10 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 11 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 12 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 13 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 14 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 15 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 16 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.56 |
| Epoch: 17 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| Epoch: 18 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| Epoch: 19 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| Epoch: 20 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.55 |
| MSE: 0.35 | RMSE: 0.59 |
Current Hyperparameters:
Batch Size: 256, Hidden Size: 64, Total Layers: 3, Dropout: 0.4, Learning Rate: 0.001
MSE: 0.3494221866130829, RMSE: 0.5911194086074829

Training model.
| Epoch: 01 | Train Loss: 0.74 | Train MSE: 0.74 | Train RMSE: 0.86 |
| Epoch: 02 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 03 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 04 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 05 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 06 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 07 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 08 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 09 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 10 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 11 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 12 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 13 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 14 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 15 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 16 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 17 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 18 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 19 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 20 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| MSE: 0.34 | RMSE: 0.58 |
Current Hyperparameters:
Batch Size: 256, Hidden Size: 64, Total Layers: 5, Dropout: 0.2, Learning Rate: 0.01
MSE: 0.3408742845058441, RMSE: 0.5838444232940674

Training model.
| Epoch: 01 | Train Loss: 0.42 | Train MSE: 0.42 | Train RMSE: 0.65 |
| Epoch: 02 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 03 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.59 |
| Epoch: 04 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 05 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 06 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |
| Epoch: 07 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 08 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 09 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 10 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 11 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 12 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 13 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| Epoch: 14 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| Epoch: 15 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55 |
| Epoch: 16 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55 |
| Epoch: 17 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.54 |
| Epoch: 18 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| Epoch: 19 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.54 |
| Epoch: 20 | Train Loss: 0.28 | Train MSE: 0.28 | Train RMSE: 0.53 |
| MSE: 0.35 | RMSE: 0.59 |
Current Hyperparameters:
Batch Size: 256, Hidden Size: 64, Total Layers: 5, Dropout: 0.2, Learning Rate: 0.001
MSE: 0.346462607383728, RMSE: 0.5886107683181763

Training model.
| Epoch: 01 | Train Loss: 0.79 | Train MSE: 0.79 | Train RMSE: 0.89 |
| Epoch: 02 | Train Loss: 0.37 | Train MSE: 0.37 | Train RMSE: 0.61 |
| Epoch: 03 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 04 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 05 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.60 |
| Epoch: 06 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 07 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 08 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 09 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 10 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 11 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 12 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 13 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 14 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 15 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 16 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 17 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 18 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 19 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.60 |
| Epoch: 20 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| MSE: 0.35 | RMSE: 0.59 |
Current Hyperparameters:
Batch Size: 256, Hidden Size: 64, Total Layers: 5, Dropout: 0.4, Learning Rate: 0.01
MSE: 0.35326409339904785, RMSE: 0.5943602323532104

Training model.
| Epoch: 01 | Train Loss: 0.42 | Train MSE: 0.42 | Train RMSE: 0.65 |
| Epoch: 02 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 03 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 04 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 05 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 06 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 07 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 08 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 09 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |
| Epoch: 10 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |
| Epoch: 11 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 12 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 13 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 14 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 15 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| Epoch: 16 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.56 |
| Epoch: 17 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.56 |
| Epoch: 18 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| Epoch: 19 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55 |
| Epoch: 20 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55 |
| MSE: 0.38 | RMSE: 0.61 |
Current Hyperparameters:
Batch Size: 256, Hidden Size: 64, Total Layers: 5, Dropout: 0.4, Learning Rate: 0.001
MSE: 0.3764362037181854, RMSE: 0.6135439872741699

Training model.
| Epoch: 01 | Train Loss: 4.02 | Train MSE: 4.02 | Train RMSE: 2.00 |
| Epoch: 02 | Train Loss: 0.50 | Train MSE: 0.50 | Train RMSE: 0.71 |
| Epoch: 03 | Train Loss: 0.50 | Train MSE: 0.50 | Train RMSE: 0.70 |
| Epoch: 04 | Train Loss: 0.47 | Train MSE: 0.47 | Train RMSE: 0.68 |
| Epoch: 05 | Train Loss: 0.45 | Train MSE: 0.45 | Train RMSE: 0.67 |
| Epoch: 06 | Train Loss: 0.43 | Train MSE: 0.43 | Train RMSE: 0.66 |
| Epoch: 07 | Train Loss: 0.41 | Train MSE: 0.41 | Train RMSE: 0.64 |
| Epoch: 08 | Train Loss: 0.39 | Train MSE: 0.39 | Train RMSE: 0.62 |
| Epoch: 09 | Train Loss: 0.39 | Train MSE: 0.39 | Train RMSE: 0.62 |
| Epoch: 10 | Train Loss: 0.38 | Train MSE: 0.38 | Train RMSE: 0.61 |
| Epoch: 11 | Train Loss: 0.37 | Train MSE: 0.37 | Train RMSE: 0.60 |
| Epoch: 12 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 13 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 14 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 15 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 16 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 17 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 18 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 19 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 20 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| MSE: 0.35 | RMSE: 0.59 |
Current Hyperparameters:
Batch Size: 256, Hidden Size: 256, Total Layers: 3, Dropout: 0.2, Learning Rate: 0.01
MSE: 0.34600746631622314, RMSE: 0.5882239937782288

Training model.
| Epoch: 01 | Train Loss: 0.57 | Train MSE: 0.57 | Train RMSE: 0.76 |
| Epoch: 02 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 03 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 04 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 05 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 06 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |
| Epoch: 07 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 08 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 09 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 10 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 11 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.56 |
| Epoch: 12 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| Epoch: 13 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| Epoch: 14 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55 |
| Epoch: 15 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| Epoch: 16 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55 |
| Epoch: 17 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.54 |
| Epoch: 18 | Train Loss: 0.28 | Train MSE: 0.28 | Train RMSE: 0.53 |
| Epoch: 19 | Train Loss: 0.28 | Train MSE: 0.28 | Train RMSE: 0.53 |
| Epoch: 20 | Train Loss: 0.28 | Train MSE: 0.28 | Train RMSE: 0.53 |
| MSE: 0.34 | RMSE: 0.59 |
Current Hyperparameters:
Batch Size: 256, Hidden Size: 256, Total Layers: 3, Dropout: 0.2, Learning Rate: 0.001
MSE: 0.34484779834747314, RMSE: 0.5872374176979065

Training model.
| Epoch: 01 | Train Loss: 4.37 | Train MSE: 4.37 | Train RMSE: 2.09 |
| Epoch: 02 | Train Loss: 0.77 | Train MSE: 0.77 | Train RMSE: 0.88 |
| Epoch: 03 | Train Loss: 0.66 | Train MSE: 0.66 | Train RMSE: 0.81 |
| Epoch: 04 | Train Loss: 0.54 | Train MSE: 0.54 | Train RMSE: 0.73 |
| Epoch: 05 | Train Loss: 0.45 | Train MSE: 0.45 | Train RMSE: 0.67 |
| Epoch: 06 | Train Loss: 0.40 | Train MSE: 0.40 | Train RMSE: 0.63 |
| Epoch: 07 | Train Loss: 0.38 | Train MSE: 0.38 | Train RMSE: 0.62 |
| Epoch: 08 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 09 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 10 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.60 |
| Epoch: 11 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 12 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 13 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 14 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 15 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 16 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 17 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 18 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 19 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 20 | Train Loss: 0.37 | Train MSE: 0.37 | Train RMSE: 0.61 |
| MSE: 0.55 | RMSE: 0.74 |
Current Hyperparameters:
Batch Size: 256, Hidden Size: 256, Total Layers: 3, Dropout: 0.4, Learning Rate: 0.01
MSE: 0.5533763766288757, RMSE: 0.7438927292823792

Training model.
| Epoch: 01 | Train Loss: 0.56 | Train MSE: 0.56 | Train RMSE: 0.75 |
| Epoch: 02 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 03 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 04 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 05 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 06 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |
| Epoch: 07 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 08 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.59 |
| Epoch: 09 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 10 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 11 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 12 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.56 |
| Epoch: 13 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 14 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.56 |
| Epoch: 15 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| Epoch: 16 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55 |
| Epoch: 17 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55 |
| Epoch: 18 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55 |
| Epoch: 19 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.54 |
| Epoch: 20 | Train Loss: 0.28 | Train MSE: 0.28 | Train RMSE: 0.53 |
| MSE: 0.42 | RMSE: 0.64 |
Current Hyperparameters:
Batch Size: 256, Hidden Size: 256, Total Layers: 3, Dropout: 0.4, Learning Rate: 0.001
MSE: 0.41522496938705444, RMSE: 0.6443794965744019

Training model.
| Epoch: 01 | Train Loss: 2.87 | Train MSE: 2.87 | Train RMSE: 1.69 |
| Epoch: 02 | Train Loss: 0.50 | Train MSE: 0.50 | Train RMSE: 0.71 |
| Epoch: 03 | Train Loss: 0.47 | Train MSE: 0.47 | Train RMSE: 0.68 |
| Epoch: 04 | Train Loss: 0.45 | Train MSE: 0.45 | Train RMSE: 0.67 |
| Epoch: 05 | Train Loss: 0.42 | Train MSE: 0.42 | Train RMSE: 0.65 |
| Epoch: 06 | Train Loss: 0.40 | Train MSE: 0.40 | Train RMSE: 0.63 |
| Epoch: 07 | Train Loss: 0.40 | Train MSE: 0.40 | Train RMSE: 0.63 |
| Epoch: 08 | Train Loss: 0.38 | Train MSE: 0.38 | Train RMSE: 0.62 |
| Epoch: 09 | Train Loss: 0.38 | Train MSE: 0.38 | Train RMSE: 0.61 |
| Epoch: 10 | Train Loss: 0.38 | Train MSE: 0.38 | Train RMSE: 0.61 |
| Epoch: 11 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 12 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 13 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 14 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 15 | Train Loss: 0.38 | Train MSE: 0.38 | Train RMSE: 0.61 |
| Epoch: 16 | Train Loss: 0.37 | Train MSE: 0.37 | Train RMSE: 0.61 |
| Epoch: 17 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 18 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 19 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 20 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| MSE: 0.34 | RMSE: 0.59 |
Current Hyperparameters:
Batch Size: 256, Hidden Size: 256, Total Layers: 5, Dropout: 0.2, Learning Rate: 0.01
MSE: 0.3446882665157318, RMSE: 0.5871015787124634

Training model.
| Epoch: 01 | Train Loss: 0.55 | Train MSE: 0.55 | Train RMSE: 0.74 |
| Epoch: 02 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 03 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 04 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 05 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |
| Epoch: 06 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 07 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 08 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 09 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 10 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.56 |
| Epoch: 11 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.56 |
| Epoch: 12 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| Epoch: 13 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.55 |
| Epoch: 14 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.55 |
| Epoch: 15 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.54 |
| Epoch: 16 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.54 |
| Epoch: 17 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.54 |
| Epoch: 18 | Train Loss: 0.28 | Train MSE: 0.28 | Train RMSE: 0.53 |
| Epoch: 19 | Train Loss: 0.27 | Train MSE: 0.27 | Train RMSE: 0.52 |
| Epoch: 20 | Train Loss: 0.27 | Train MSE: 0.27 | Train RMSE: 0.52 |
| MSE: 0.33 | RMSE: 0.58 |
Current Hyperparameters:
Batch Size: 256, Hidden Size: 256, Total Layers: 5, Dropout: 0.2, Learning Rate: 0.001
MSE: 0.33400189876556396, RMSE: 0.5779289603233337

Training model.
| Epoch: 01 | Train Loss: 2.52 | Train MSE: 2.52 | Train RMSE: 1.59 |
| Epoch: 02 | Train Loss: 0.59 | Train MSE: 0.59 | Train RMSE: 0.77 |
| Epoch: 03 | Train Loss: 0.48 | Train MSE: 0.48 | Train RMSE: 0.69 |
| Epoch: 04 | Train Loss: 0.41 | Train MSE: 0.41 | Train RMSE: 0.64 |
| Epoch: 05 | Train Loss: 0.37 | Train MSE: 0.37 | Train RMSE: 0.61 |
| Epoch: 06 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 07 | Train Loss: 0.37 | Train MSE: 0.37 | Train RMSE: 0.61 |
| Epoch: 08 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 09 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 10 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 11 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 12 | Train Loss: 0.38 | Train MSE: 0.38 | Train RMSE: 0.62 |
| Epoch: 13 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 14 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 15 | Train Loss: 0.37 | Train MSE: 0.37 | Train RMSE: 0.61 |
| Epoch: 16 | Train Loss: 0.37 | Train MSE: 0.37 | Train RMSE: 0.61 |
| Epoch: 17 | Train Loss: 0.36 | Train MSE: 0.36 | Train RMSE: 0.60 |
| Epoch: 18 | Train Loss: 0.38 | Train MSE: 0.38 | Train RMSE: 0.62 |
| Epoch: 19 | Train Loss: 0.42 | Train MSE: 0.42 | Train RMSE: 0.65 |
| Epoch: 20 | Train Loss: 0.37 | Train MSE: 0.37 | Train RMSE: 0.60 |
| MSE: 0.35 | RMSE: 0.59 |
Current Hyperparameters:
Batch Size: 256, Hidden Size: 256, Total Layers: 5, Dropout: 0.4, Learning Rate: 0.01
MSE: 0.3506881892681122, RMSE: 0.5921893119812012

Training model.
| Epoch: 01 | Train Loss: 0.54 | Train MSE: 0.54 | Train RMSE: 0.74 |
| Epoch: 02 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 03 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 04 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 05 | Train Loss: 0.35 | Train MSE: 0.35 | Train RMSE: 0.59 |
| Epoch: 06 | Train Loss: 0.34 | Train MSE: 0.34 | Train RMSE: 0.58 |
| Epoch: 07 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |
| Epoch: 08 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |
| Epoch: 09 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |
| Epoch: 10 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |
| Epoch: 11 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.56 |
| Epoch: 12 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.56 |
| Epoch: 13 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |
| Epoch: 14 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |
| Epoch: 15 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.56 |
| Epoch: 16 | Train Loss: 0.30 | Train MSE: 0.30 | Train RMSE: 0.55 |
| Epoch: 17 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.56 |
| Epoch: 18 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.54 |
| Epoch: 19 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.53 |
| Epoch: 20 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.53 |
| MSE: 0.36 | RMSE: 0.60 |
Current Hyperparameters:
Batch Size: 256, Hidden Size: 256, Total Layers: 5, Dropout: 0.4, Learning Rate: 0.001
MSE: 0.36257678270339966, RMSE: 0.6021434664726257

Best Hyperparameters and Metrics
Batch Size: 64, Hidden Size: 64, Total Layers: 3, Dropout: 0.2, Learning Rate: 0.01
MSE: 0.31465229392051697, RMSE: 0.5609387755393982
 23:53:00 up 29 days,  8:09,  2 users,  load average: 2.08, 2.14, 2.16
