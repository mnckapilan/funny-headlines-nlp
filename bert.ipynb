{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import torch.optim as optim\n",
    "import codecs\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-12 19:24:45--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2021-02-12 19:24:45--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2021-02-12 19:24:45--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M   796KB/s    in 10m 41s \n",
      "\n",
      "2021-02-12 19:35:27 (1.28 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n",
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/task-2/train.csv')\n",
    "test_df = pd.read_csv('data/task-2/dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(series):\n",
    "    tokenized = []\n",
    "    for sentence in tqdm(series):\n",
    "        blob = TextBlob(sentence)\n",
    "        words = blob.words.lower()\n",
    "        lemmas = words.lemmatize()\n",
    "        tokenized.append(lemmas)\n",
    "\n",
    "    return pd.Series(tokenized)\n",
    "\n",
    "def create_edited_sentences(data, edits):\n",
    "    edited_sentences = []\n",
    "    for i, headline in enumerate(data):\n",
    "        start_loc = headline.find('<')\n",
    "        end_loc = headline.find('/>')\n",
    "        edited_sentences.append(headline[:start_loc] + edits[i] + headline[end_loc+2:])\n",
    "    return pd.Series(edited_sentences)\n",
    "\n",
    "def remove_tags_sentences(data):\n",
    "    clean_sentences = []\n",
    "    for _, headline in enumerate(data):\n",
    "        clean_sentences.append(headline.replace('<','').replace('/>',''))\n",
    "    return pd.Series(clean_sentences)\n",
    "\n",
    "def get_max_len_sentence(data):\n",
    "    return data.map(len).max()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9381/9381 [00:02<00:00, 4060.07it/s]\n",
      "100%|██████████| 9381/9381 [00:02<00:00, 3302.36it/s]\n",
      "100%|██████████| 9381/9381 [00:02<00:00, 3544.77it/s]\n",
      "100%|██████████| 9381/9381 [00:02<00:00, 4153.88it/s]\n",
      "100%|██████████| 2355/2355 [00:00<00:00, 3436.27it/s]\n",
      "100%|██████████| 2355/2355 [00:00<00:00, 3396.46it/s]\n",
      "100%|██████████| 2355/2355 [00:00<00:00, 3471.46it/s]\n",
      "100%|██████████| 2355/2355 [00:00<00:00, 3667.03it/s]\n"
     ]
    }
   ],
   "source": [
    "train_original_1 = remove_tags_sentences(train_df['original1'])\n",
    "train_edits_1 = train_df['edit1']\n",
    "train_replaced_1 = create_edited_sentences(train_df['original1'], train_edits_1)\n",
    "train_tokenized_original_1 = preprocess(train_original_1)\n",
    "train_tokenized_replaced_1 = preprocess(train_replaced_1)\n",
    "\n",
    "train_original_2 = remove_tags_sentences(train_df['original2'])\n",
    "train_edits_2 = train_df['edit2']\n",
    "train_replaced_2 = create_edited_sentences(train_df['original2'], train_edits_2)\n",
    "train_tokenized_original_2 = preprocess(train_original_2)\n",
    "train_tokenized_replaced_2 = preprocess(train_replaced_2)\n",
    "\n",
    "test_original_1 = remove_tags_sentences(test_df['original1'])\n",
    "test_edits_1 = test_df['edit1']\n",
    "test_replaced_1 = create_edited_sentences(test_df['original1'], test_edits_1)\n",
    "test_tokenized_original_1 = preprocess(test_original_1)\n",
    "test_tokenized_replaced_1 = preprocess(test_replaced_1)\n",
    "\n",
    "test_original_2 = remove_tags_sentences(test_df['original2'])\n",
    "test_edits_2 = test_df['edit2']\n",
    "test_replaced_2 = create_edited_sentences(test_df['original2'], test_edits_2)\n",
    "test_tokenized_original_2 = preprocess(test_original_2)\n",
    "test_tokenized_replaced_2 = preprocess(test_replaced_2)\n",
    "\n",
    "max_sentence_length = get_max_len_sentence(train_tokenized_original_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [gene, cernan, last, astronaut, on, the, moon,...\n",
      "1        [gene, cernan, last, astronaut, on, the, moon,...\n",
      "2        [gene, cernan, last, dancer, on, the, moon, dy...\n",
      "3        [i, 'm, done, fed, up, with, california, some,...\n",
      "4        [i, 'm, done, fed, up, with, pancake, some, co...\n",
      "                               ...                        \n",
      "28138    [“, kompromat, ”, medium, ethic, and, the, law...\n",
      "28139    [“, kompromat, ”, medium, ethic, and, the, law...\n",
      "28140    [“, son, of, a, bitch, ”, trump, ’, s, nfl, ta...\n",
      "28141    [“, son, of, a, bitch, ”, trump, ’, s, nfl, ou...\n",
      "28142    [“, son, of, a, father, ”, trump, ’, s, nfl, o...\n",
      "Length: 28143, dtype: object\n",
      "0        \" Gene Cernan , Last Astronaut on the Moon , D...\n",
      "1        \" Gene Cernan , Last Astronaut on the Moon , i...\n",
      "2        \" Gene Cernan , Last Dancer on the Moon , Dies...\n",
      "3        \" I 'm done \" : Fed up with California , some ...\n",
      "4        \" I 'm done \" : Fed up with pancakes , some co...\n",
      "                               ...                        \n",
      "28138    “ Kompromat , ” media ethics and the law : Wha...\n",
      "28139    “ Kompromat , ” media ethics and the law : Wha...\n",
      "28140    “ Son of a bitch ” : Trump ’s NFL tantrum fits...\n",
      "28141    “ Son of a bitch ” : Trump ’s NFL outburst fit...\n",
      "28142    “ Son of a father ” : Trump ’s NFL outburst fi...\n",
      "Length: 28143, dtype: object\n"
     ]
    }
   ],
   "source": [
    "train_tokenized = pd.concat([train_tokenized_original_1, train_tokenized_replaced_1, train_tokenized_replaced_2]).sort_index().reset_index(drop=True)\n",
    "print(train_tokenized)\n",
    "\n",
    "train_sentences = pd.concat([train_original_1, train_replaced_1, train_replaced_2]).sort_index().reset_index(drop=True)\n",
    "print(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9381/9381 [00:00<00:00, 388138.52it/s]\n",
      "100%|██████████| 9381/9381 [00:00<00:00, 246032.61it/s]\n",
      "100%|██████████| 9381/9381 [00:00<00:00, 366770.44it/s]\n",
      "100%|██████████| 2355/2355 [00:00<00:00, 191958.06it/s]\n",
      "100%|██████████| 2355/2355 [00:00<00:00, 197255.83it/s]\n",
      "100%|██████████| 2355/2355 [00:00<00:00, 312670.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def add_to_vocab(vocab, series):\n",
    "    for sentence in tqdm(series):\n",
    "        for word in sentence:\n",
    "            if(word == \"dropuppiesarkets\"):\n",
    "                print('what the heck')\n",
    "            vocab.add(word)\n",
    "\n",
    "vocab = set()\n",
    "add_to_vocab(vocab, train_tokenized_original_1)\n",
    "add_to_vocab(vocab, train_tokenized_replaced_1)\n",
    "add_to_vocab(vocab, train_tokenized_replaced_2)\n",
    "\n",
    "add_to_vocab(vocab, test_tokenized_original_1)\n",
    "add_to_vocab(vocab, test_tokenized_replaced_1)\n",
    "add_to_vocab(vocab, test_tokenized_replaced_2)\n",
    "\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"obamacare\" in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [00:11<00:00, 36065.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# We create representations for our tokens\n",
    "wvecs = [] # word vectors\n",
    "word2idx = [] # word2index\n",
    "idx2word = []\n",
    "\n",
    "# This is a large file, it will take a while to load in the memory!\n",
    "with codecs.open('glove.6B.300d.txt', 'r','utf-8') as f:\n",
    "  index = 1\n",
    "  for line in tqdm(f.readlines()):\n",
    "    # Ignore the first line - first line typically contains vocab, dimensionality\n",
    "    if len(line.strip().split()) > 3:\n",
    "      word = line.strip().split()[0]\n",
    "      if word in vocab:\n",
    "          word, vec = (word,\n",
    "                     list(map(float,line.strip().split()[1:])))\n",
    "          wvecs.append(vec)\n",
    "          word2idx.append((word, index))\n",
    "          idx2word.append((index, word))\n",
    "          index += 1\n",
    "\n",
    "\n",
    "wvecs = np.array(wvecs)\n",
    "word2idx = dict(word2idx)\n",
    "idx2word = dict(idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define our training loop\n",
    "def train(train_iter, dev_iter, model, number_epoch):\n",
    "    \"\"\"\n",
    "    Training loop for the model, which calls on eval to evaluate after each epoch\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Training model.\")\n",
    "\n",
    "    for epoch in range(1, number_epoch+1):\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        epoch_correct = 0\n",
    "        no_observations = 0  # Observations used for training so far\n",
    "\n",
    "        for batch in train_iter:\n",
    "            feature, target = batch\n",
    "            feature, target = feature.to(device), target.to(device)\n",
    "\n",
    "            # for RNN:\n",
    "            model.batch_size = target.shape[0]\n",
    "            no_observations = no_observations + target.shape[0]\n",
    "            model.hidden = model.init_hidden()\n",
    "\n",
    "            predictions = model(feature).squeeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(predictions, target)\n",
    "\n",
    "            correct, __ = model_performance(np.argmax(predictions.detach().cpu().numpy(), axis=1), target.detach().cpu().numpy())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()*target.shape[0]\n",
    "            epoch_correct += correct\n",
    "\n",
    "        valid_loss, valid_acc, __, __ = eval(dev_iter, model)\n",
    "\n",
    "        epoch_loss, epoch_acc = epoch_loss / no_observations, epoch_correct / no_observations\n",
    "        print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.2f} | Train Accuracy: {epoch_acc:.2f} | \\\n",
    "        Val. Loss: {valid_loss:.2f} | Val. Accuracy: {valid_acc:.2f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We evaluate performance on our dev set\n",
    "def eval(data_iter, model):\n",
    "    \"\"\"\n",
    "    Evaluating model performance on the dev set\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    pred_all = []\n",
    "    trg_all = []\n",
    "    no_observations = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_iter:\n",
    "            feature, target = batch\n",
    "\n",
    "            feature, target = feature.to(device), target.to(device)\n",
    "\n",
    "            # for RNN:\n",
    "            model.batch_size = target.shape[0]\n",
    "            no_observations = no_observations + target.shape[0]\n",
    "            model.hidden = model.init_hidden()\n",
    "\n",
    "            predictions = model(feature).squeeze(1)\n",
    "            loss = loss_fn(predictions, target)\n",
    "\n",
    "            # We get the mse\n",
    "            pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n",
    "            correct, __ = model_performance(np.argmax(pred, axis=1), trg)\n",
    "\n",
    "            epoch_loss += loss.item()*target.shape[0]\n",
    "            epoch_correct += correct\n",
    "            pred_all.extend(pred)\n",
    "            trg_all.extend(trg)\n",
    "\n",
    "    return epoch_loss/no_observations, epoch_correct/no_observations, np.array(pred_all), np.array(trg_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How we print the model performance\n",
    "def model_performance(output, target, print_output=False):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    correct_answers = (output == target)\n",
    "    correct = sum(correct_answers)\n",
    "    acc = np.true_divide(correct,len(output))\n",
    "\n",
    "    if print_output:\n",
    "        print(f'| Acc: {acc:.2f} ')\n",
    "\n",
    "    return correct, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for collating our observations into minibatches:\n",
    "def collate_fn_padd(batch):\n",
    "    '''\n",
    "    We add padding to our minibatches and create tensors for our model\n",
    "    '''\n",
    "\n",
    "    batch_labels = [l for f, l in batch]\n",
    "    batch_features = [f for f, l in batch]\n",
    "\n",
    "    batch_features_len = [len(f) for f, l in batch]\n",
    "\n",
    "    seq_tensor = torch.zeros((len(batch), max(batch_features_len))).long()\n",
    "\n",
    "    for idx, (seq, seqlen) in enumerate(zip(batch_features, batch_features_len)):\n",
    "        seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "\n",
    "    batch_labels = torch.LongTensor(batch_labels)\n",
    "\n",
    "    return seq_tensor, batch_labels\n",
    "\n",
    "# We create a Dataset so we can create minibatches\n",
    "class Task2Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, train_data, labels):\n",
    "        self.x_train = train_data\n",
    "        self.y_train = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.x_train[item], self.y_train[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_classification(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, batch_size, device):\n",
    "        super(BiLSTM_classification, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2label = nn.Linear(hidden_dim * 2, 3)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers * num_directions, minibatch_size, hidden_dim)\n",
    "        return torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device), \\\n",
    "               torch.zeros(2, self.batch_size, self.hidden_dim).to(self.device)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embedded = self.embedding(sentence)\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embedded.view(len(embedded), self.batch_size, self.embedding_dim), self.hidden)\n",
    "\n",
    "        out = self.hidden2label(lstm_out[-1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialised.\n",
      "Dataloaders created.\n",
      "Training model.\n",
      "| Epoch: 01 | Train Loss: 0.96 | Train Accuracy: 0.45 |         Val. Loss: 0.98 | Val. Accuracy: 0.44 |\n",
      "| Epoch: 02 | Train Loss: 0.96 | Train Accuracy: 0.44 |         Val. Loss: 0.97 | Val. Accuracy: 0.44 |\n",
      "| Epoch: 03 | Train Loss: 0.95 | Train Accuracy: 0.48 |         Val. Loss: 0.97 | Val. Accuracy: 0.47 |\n",
      "| Epoch: 04 | Train Loss: 0.88 | Train Accuracy: 0.60 |         Val. Loss: 1.00 | Val. Accuracy: 0.48 |\n",
      "| Epoch: 05 | Train Loss: 0.81 | Train Accuracy: 0.66 |         Val. Loss: 1.05 | Val. Accuracy: 0.48 |\n",
      "| Epoch: 06 | Train Loss: 0.74 | Train Accuracy: 0.68 |         Val. Loss: 1.12 | Val. Accuracy: 0.48 |\n",
      "| Epoch: 07 | Train Loss: 0.66 | Train Accuracy: 0.69 |         Val. Loss: 1.28 | Val. Accuracy: 0.48 |\n",
      "| Epoch: 08 | Train Loss: 0.59 | Train Accuracy: 0.70 |         Val. Loss: 1.40 | Val. Accuracy: 0.47 |\n",
      "| Epoch: 09 | Train Loss: 0.55 | Train Accuracy: 0.70 |         Val. Loss: 1.63 | Val. Accuracy: 0.47 |\n",
      "| Epoch: 10 | Train Loss: 0.52 | Train Accuracy: 0.71 |         Val. Loss: 1.81 | Val. Accuracy: 0.46 |\n"
     ]
    }
   ],
   "source": [
    "# Number of epochs\n",
    "epochs = 10\n",
    "\n",
    "# Proportion of training data for train compared to dev\n",
    "train_proportion = 0.8\n",
    "\n",
    "vectorized_seqs = [[word2idx[tok] for tok in seq if tok in word2idx] for seq in train_tokenized_original_1]\n",
    "\n",
    "INPUT_DIM = len(word2idx)\n",
    "EMBEDDING_DIM = 300\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "device = \"cpu\"\n",
    "model = BiLSTM_classification(EMBEDDING_DIM, 50, INPUT_DIM, BATCH_SIZE, device)\n",
    "print(\"Model initialised.\")\n",
    "\n",
    "model.to(device)\n",
    "# We provide the model with our embeddings\n",
    "model.embedding.weight.data.copy_(torch.from_numpy(wvecs))\n",
    "\n",
    "feature = vectorized_seqs\n",
    "\n",
    "# 'feature' is a list of lists, each containing embedding IDs for word tokens\n",
    "train_and_dev = Task2Dataset(feature, train_df['label'])\n",
    "\n",
    "train_examples = round(len(train_and_dev)*train_proportion)\n",
    "dev_examples = len(train_and_dev) - train_examples\n",
    "\n",
    "train_dataset, dev_dataset = random_split(train_and_dev,\n",
    "                                           (train_examples,\n",
    "                                            dev_examples))\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\n",
    "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\n",
    "\n",
    "print(\"Dataloaders created.\")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn = loss_fn.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train(train_loader, dev_loader, model, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9381 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|██████████| 9381/9381 [00:03<00:00, 3078.72it/s]\n",
      "100%|██████████| 9381/9381 [00:03<00:00, 2894.83it/s]\n",
      "100%|██████████| 9381/9381 [00:02<00:00, 3517.23it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "def tokenize_sentence(data):\n",
    "    tokenized = []\n",
    "    attention_masks = []\n",
    "    for sentence in tqdm(train_original_1):\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_sentence_length,\n",
    "            pad_to_max_length=True\n",
    "        )\n",
    "\n",
    "        tokenized.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "        \n",
    "    return torch.tensor(tokenized), torch.tensor(attention_masks)\n",
    "        \n",
    "original_tokenized, original_attention_masks = tokenize_sentence(train_original_1)\n",
    "replaced_1_tokenized, replaced_1_attention_masks = tokenize_sentence(train_replaced_1)\n",
    "replaced_2_tokenized, replaced_2_attention_masks = tokenize_sentence(train_replaced_2)\n",
    "\n",
    "train_tokenized = torch.cat((original_tokenized, replaced_1_tokenized, replaced_2_tokenized), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_tokenized, shuffle=True, batch_size=BATCH_SIZE)\n",
    "# dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 2\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
