{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "bert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bearing-daniel"
      },
      "source": [
        "# Preprocessing"
      ],
      "id": "bearing-daniel"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdWQUNh3hueD",
        "outputId": "19276f83-0182-46d0-cb5b-632fbe1f9776"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "fdWQUNh3hueD",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMFp_-PwylIW",
        "outputId": "c4fe533c-c615-465a-d605-f70627a19256"
      },
      "source": [
        "!pip install transformers"
      ],
      "id": "CMFp_-PwylIW",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tb7pA8y9iRpU",
        "outputId": "fabe9326-248c-405d-bb86-74f86c957cd2"
      },
      "source": [
        "%cd '/content/drive/MyDrive/Colab Notebooks/NLPGroupCW'"
      ],
      "id": "Tb7pA8y9iRpU",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1qcQshV1qoMFlpNzFTKlWvvjZbTNz5V3t/NLPGroupCW\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "green-hostel"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "from dataloaders import *\n",
        "from processor import *"
      ],
      "id": "green-hostel",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "institutional-burner"
      },
      "source": [
        "# Setting random seed and device\n",
        "SEED = 1\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
      ],
      "id": "institutional-burner",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "democratic-letters"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "vocab_size = len(tokenizer.vocab)\n",
        "\n",
        "max_sentence_length = tokenizer.max_model_input_sizes['bert-base-uncased']"
      ],
      "id": "democratic-letters",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "medieval-convertible"
      },
      "source": [
        "def tokenize(corpus):\n",
        "    return [tokenizer.tokenize(sentence) for sentence in corpus]\n",
        "\n",
        "def to_ids(corpus):\n",
        "    return [tokenizer.convert_tokens_to_ids(sentence) for sentence in corpus]"
      ],
      "id": "medieval-convertible",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "destroyed-internet"
      },
      "source": [
        "train_df = pd.read_csv('data/task-1/train.csv')\n",
        "test_df = pd.read_csv('data/task-1/dev.csv')\n",
        "\n",
        "training_data = train_df['original']\n",
        "training_edits = train_df['edit']\n",
        "test_data = test_df['original']\n",
        "test_edits = test_df['edit']\n",
        "\n",
        "training_grades = train_df['meanGrade']\n",
        "\n",
        "edited_training = pd.Series(create_edited_sentences(training_data, training_edits))\n",
        "edited_test = pd.Series(create_edited_sentences(test_data, test_edits))"
      ],
      "id": "destroyed-internet",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apparent-hopkins"
      },
      "source": [
        "training_tokens = tokenize(edited_training)\n",
        "testing_tokens = tokenize(edited_test)\n",
        "\n",
        "training_ids = to_ids(training_tokens)\n",
        "testing_tokens = to_ids(testing_tokens)"
      ],
      "id": "apparent-hopkins",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "informal-protest",
        "outputId": "78e2f7b5-7270-4e2d-8556-2ebeef8d6c64"
      },
      "source": [
        "print(training_tokens[100])\n",
        "print(training_ids[100])"
      ],
      "id": "informal-protest",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['trump', 'asked', 'du', '##ter', '##te', 'if', 'philippines', 'has', 'death', 'limit', ',', 'philippines', 'ambassador', 'says']\n",
            "[8398, 2356, 4241, 3334, 2618, 2065, 5137, 2038, 2331, 5787, 1010, 5137, 6059, 2758]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "banned-shepherd"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train = Task1Dataset(training_ids, training_grades)\n",
        "train_dataset, validation_dataset = dataset_split(train)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)"
      ],
      "id": "banned-shepherd",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "practical-creek"
      },
      "source": [
        "# Build Model"
      ],
      "id": "practical-creek"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sorted-tomato"
      },
      "source": [
        "bert_model = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "id": "sorted-tomato",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "searching-winner"
      },
      "source": [
        "class BertGradePredictor(nn.Module):\n",
        "    def __init__(self, bert_model, total_layers, hid_size, out_size, isBidir, drop):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.bert_model = bert_model\n",
        "\n",
        "        self.isBidir = isBidir\n",
        "        \n",
        "        embed_size = bert.config.to_dict()['hidden_size']\n",
        "\n",
        "        if total_layers < 3:\n",
        "          drop = 0\n",
        "\n",
        "        hid_output_size = hid_size\n",
        "        if isBidir:\n",
        "          hid_output_size = hid_output_size * 2\n",
        "        \n",
        "        self.drop = drop\n",
        "\n",
        "        self.gru = nn.GRU(input_size=embed_size,\n",
        "                          hidden_size=hid_size,\n",
        "                          num_layers=total_layers,\n",
        "                          bidirectional=isBidir,\n",
        "                          batch_first=True,\n",
        "                          dropout=drop)\n",
        "        \n",
        "        self.fc1 = nn.Linear(hid_output_size, out_size)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "\n",
        "        isBidir = self.isBidir\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            x_embed = self.bert_model(x)\n",
        "            x_embed = x_embed[0]\n",
        "        \n",
        "        cell, hid = self.gru(x_embed)\n",
        "        hid_last = hid[-1,:,:]\n",
        "        hid_snd_last = hid[-2,:,:]\n",
        "        \n",
        "        if isBidir:\n",
        "            hid = F.dropout(torch.cat((hid_snd_last, hid_last), dim=1), self.drop)\n",
        "        else:\n",
        "            hid = F.dropout(hid_last, self.drop)\n",
        "        \n",
        "        out = self.fc1(hid)\n",
        "        \n",
        "        return out"
      ],
      "id": "searching-winner",
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkkZ8FHuui03"
      },
      "source": [
        "total_layers = 3\r\n",
        "hid_size = 128\r\n",
        "out_size = 1\r\n",
        "drop = 0.3\r\n",
        "isBidir = True"
      ],
      "id": "IkkZ8FHuui03",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loving-wilderness"
      },
      "source": [
        "model = BertGradePredictor(bert_model,\n",
        "                         total_layers,\n",
        "                         hid_size,\n",
        "                         out_size,\n",
        "                         isBidir,\n",
        "                         drop)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "model = model.to(device)\n",
        "loss_fn = loss_fn.to(device)"
      ],
      "id": "loving-wilderness",
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "comprehensive-grenada"
      },
      "source": [
        "bert_layers = model.named_parameters()\n",
        "bert_layers = [(layer, parameter) for layer, parameter in bert_layers]\n",
        "for i in range(len(bert_layers)):\n",
        "    layer_p = bert_layers[i]\n",
        "    layer = layer_p[0]\n",
        "    p = layer_p[1]\n",
        "    if \"bert_model\" in layer:\n",
        "      p.requires_grad = False"
      ],
      "id": "comprehensive-grenada",
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "polyphonic-garlic"
      },
      "source": [
        "def model_performance(output, target, print_output=False):\n",
        "    \"\"\"\n",
        "    Returns SSE and MSE per batch (printing the MSE and the RMSE)\n",
        "    \"\"\"\n",
        "\n",
        "    sq_error = (output - target)**2\n",
        "\n",
        "    sse = np.sum(sq_error)\n",
        "    mse = np.mean(sq_error)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    if print_output:\n",
        "        print(f'| MSE: {mse:.2f} | RMSE: {rmse:.2f} |')\n",
        "\n",
        "    return sse, mse"
      ],
      "id": "polyphonic-garlic",
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "curious-delhi"
      },
      "source": [
        "def eval(data_iter, model):\n",
        "    \"\"\"\n",
        "    Evaluating model performance on the dev set\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_sse = 0\n",
        "    pred_all = []\n",
        "    trg_all = []\n",
        "    no_observations = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_iter:\n",
        "            feature, target = batch\n",
        "            feature, target = feature.to(device), target.to(device)\n",
        "            # for RNN:\n",
        "            # model.batch_size = target.shape[0]\n",
        "            no_observations = no_observations + target.shape[0]\n",
        "            # model.hidden = model.init_hidden()\n",
        "\n",
        "            predictions = model(feature).squeeze(1)\n",
        "            loss = loss_fn(predictions, target)\n",
        "\n",
        "            # We get the mse\n",
        "            pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n",
        "            sse, __ = model_performance(pred, trg)\n",
        "\n",
        "            epoch_loss += loss.item()*target.shape[0]\n",
        "            epoch_sse += sse\n",
        "            pred_all.extend(pred)\n",
        "            trg_all.extend(trg)\n",
        "\n",
        "    return epoch_loss/no_observations, epoch_sse/no_observations, np.array(pred_all), np.array(trg_all)"
      ],
      "id": "curious-delhi",
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "documented-google"
      },
      "source": [
        "def train(train_loader, validation_loader, model, number_epoch):\n",
        "    \"\"\"\n",
        "    Training loop for the model, which calls on eval to evaluate after each epoch\n",
        "    \"\"\"\n",
        "    print(\"Training model.\")\n",
        "    for epoch in range(1, number_epoch+1):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        epoch_sse = 0\n",
        "        no_observations = 0  # Observations used for training so far\n",
        "        for batch in train_loader:\n",
        "            feature, target = batch\n",
        "            feature, target = feature.to(device), target.to(device)\n",
        "            # for RNN:\n",
        "            # model.batch_size = target.shape[0]\n",
        "            no_observations = no_observations + target.shape[0]\n",
        "            # model.hidden = model.init_hidden()\n",
        "            predictions = model(feature).squeeze(1)\n",
        "            optimizer.zero_grad()\n",
        "            loss = loss_fn(predictions, target)\n",
        "            sse, __ = model_performance(predictions.detach().cpu().numpy(), target.detach().cpu().numpy())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()*target.shape[0]\n",
        "            epoch_sse += sse\n",
        "\n",
        "        valid_loss, valid_mse, __, __ = eval(validation_loader, model)\n",
        "\n",
        "        epoch_loss, epoch_mse = epoch_loss / no_observations, epoch_sse / no_observations\n",
        "        print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.2f} | Train MSE: {epoch_mse:.2f} | Train RMSE: {epoch_mse**0.5:.2f} | \\\n",
        "        Val. Loss: {valid_loss:.2f} | Val. MSE: {valid_mse:.2f} |  Val. RMSE: {valid_mse**0.5:.2f} |')"
      ],
      "id": "documented-google",
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acquired-terrorism",
        "outputId": "87264e45-aa07-4999-ccfc-ed6c9fdd2ed6"
      },
      "source": [
        "train(train_loader, validation_loader, model, 2)"
      ],
      "id": "acquired-terrorism",
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model.\n",
            "| Epoch: 01 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |         Val. Loss: 0.37 | Val. MSE: 0.37 |  Val. RMSE: 0.61 |\n",
            "| Epoch: 02 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.57 |         Val. Loss: 0.33 | Val. MSE: 0.33 |  Val. RMSE: 0.57 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9pBMkH3i0Sz"
      },
      "source": [
        ""
      ],
      "id": "T9pBMkH3i0Sz",
      "execution_count": null,
      "outputs": []
    }
  ]
}