{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kapilan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "from nltk.corpus import stopwords\n",
    "from processor import create_edited_sentences, lookup_glove\n",
    "import codecs\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyphen_tokenizer(data):\n",
    "    tokenized_corpus = []  # Let us put the tokenized corpus in a list\n",
    "    for sentence in data:\n",
    "        sentence = sentence.lower()\n",
    "        tokenized_sentence = []\n",
    "        for token in sentence.split(' '):  # simplest split is\n",
    "            for tok in token.split('-'):\n",
    "                tokenized_sentence.append(tok)\n",
    "        tokenized_corpus.append(tokenized_sentence)\n",
    "    # Create single list of all vocabulary\n",
    "    vocabulary = []  # Let us put all the tokens (mostly words) appearing in the vocabulary in a list\n",
    "    for sentence in tokenized_corpus:\n",
    "        for token in sentence:\n",
    "            if token not in vocabulary:\n",
    "                if True:\n",
    "                    vocabulary.append(token)\n",
    "    return vocabulary, tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def basic_tokenizer(data):\n",
    "    tokenized_corpus = []  # Let us put the tokenized corpus in a list\n",
    "    for sentence in data:\n",
    "        sentence = sentence.lower()\n",
    "        tokenized_sentence = []\n",
    "        for token in sentence.split(' '):  # simplest split is\n",
    "            tokenized_sentence.append(token)\n",
    "        tokenized_corpus.append(tokenized_sentence)\n",
    "    # Create single list of all vocabulary\n",
    "    vocabulary = []  # Let us put all the tokens (mostly words) appearing in the vocabulary in a list\n",
    "    for sentence in tokenized_corpus:\n",
    "        for token in sentence:\n",
    "            if token not in vocabulary:\n",
    "                if True:\n",
    "                    vocabulary.append(token)\n",
    "    return vocabulary, tokenized_corpus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def hyphen_tokenizer_no_numbers(data):\n",
    "    tokenized_corpus = []  # Let us put the tokenized corpus in a list\n",
    "    for sentence in data:\n",
    "        sentence = sentence.lower()\n",
    "        tokenized_sentence = []\n",
    "        for token in sentence.split(' '):  # simplest split is\n",
    "            for tok in token.split('-'):\n",
    "                tok = re.sub(\"\\d+\", \"\", tok)\n",
    "                tokenized_sentence.append(tok)\n",
    "        tokenized_corpus.append(tokenized_sentence)\n",
    "    # Create single list of all vocabulary\n",
    "    vocabulary = []  # Let us put all the tokens (mostly words) appearing in the vocabulary in a list\n",
    "    for sentence in tokenized_corpus:\n",
    "        for token in sentence:\n",
    "            if token not in vocabulary:\n",
    "                if True:\n",
    "                    vocabulary.append(token)\n",
    "    return vocabulary, tokenized_corpus\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/task-1/train.csv')\n",
    "test_df = pd.read_csv('data/task-1/dev.csv')\n",
    "\n",
    "training_data = train_df['original']\n",
    "training_edits = train_df['edit']\n",
    "test_data = test_df['original']\n",
    "test_edits = test_df['edit']\n",
    "\n",
    "edited_training = pd.Series(create_edited_sentences(training_data, training_edits))\n",
    "edited_test = pd.Series(create_edited_sentences(test_data, test_edits))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def build_glove_dictionary(embedding_dim):\n",
    "    word2embedding = {}\n",
    "    with codecs.open('model-downloads/glove.twitter.27B/glove.twitter.27B.{}d.txt'.format(embedding_dim), 'r', 'utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            if len(line.strip().split()) > 3:\n",
    "                word = line.strip().split()[0]\n",
    "                word2embedding[word] = np.array(list(map(float, line.strip().split()[1:])))\n",
    "\n",
    "    return word2embedding\n",
    "\n",
    "def build_embedding_tensor(vocab, embedding_dim=50):\n",
    "    glove_vectors = np.zeros((len(vocab) + 1, embedding_dim))\n",
    "    word2embedding = build_glove_dictionary(embedding_dim)\n",
    "    words_not_in_glove = 0\n",
    "    for i, word in enumerate(vocab):\n",
    "        glove_vec, in_glove = lookup_glove(word2embedding, word, embedding_dim)\n",
    "        glove_vectors[i + 1] = glove_vec\n",
    "        words_not_in_glove += in_glove\n",
    "    print(\"Number of words not in GloVe: {}\".format(words_not_in_glove), flush=True)\n",
    "\n",
    "    return torch.from_numpy(glove_vectors).type(torch.float32), words_not_in_glove"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def stemming(data):\n",
    "    tokenized_corpus = []  # Let us put the tokenized corpus in a list\n",
    "    for sentence in data:\n",
    "        sentence = sentence.lower()\n",
    "        tokenized_sentence = []\n",
    "        for token in sentence.split(' '):  # simplest split is\n",
    "            for tok in token.split('-'):\n",
    "                tok = re.sub(\"\\d+\", \"\", tok)\n",
    "                tok = porter.stem(tok)\n",
    "                tokenized_sentence.append(tok)\n",
    "        tokenized_corpus.append(tokenized_sentence)\n",
    "    # Create single list of all vocabulary\n",
    "    vocabulary = []  # Let us put all the tokens (mostly words) appearing in the vocabulary in a list\n",
    "    for sentence in tokenized_corpus:\n",
    "        for token in sentence:\n",
    "            if token not in vocabulary:\n",
    "                if True:\n",
    "                    vocabulary.append(token)\n",
    "    return vocabulary, tokenized_corpus\n",
    "\n",
    "def lemmatize(data):\n",
    "    tokenized_corpus = []  # Let us put the tokenized corpus in a list\n",
    "    for sentence in data:\n",
    "        sentence = sentence.lower()\n",
    "        tokenized_sentence = []\n",
    "        for token in sentence.split(' '):  # simplest split is\n",
    "            for tok in token.split('-'):\n",
    "                tok = re.sub(\"\\d+\", \"\", tok)\n",
    "                tok = wordnet_lemmatizer.lemmatize(tok)\n",
    "                tokenized_sentence.append(tok)\n",
    "        tokenized_corpus.append(tokenized_sentence)\n",
    "    # Create single list of all vocabulary\n",
    "    vocabulary = []  # Let us put all the tokens (mostly words) appearing in the vocabulary in a list\n",
    "    for sentence in tokenized_corpus:\n",
    "        for token in sentence:\n",
    "            if token not in vocabulary:\n",
    "                if True:\n",
    "                    vocabulary.append(token)\n",
    "    return vocabulary, tokenized_corpus\n",
    "\n",
    "def remove_stopwords(data):\n",
    "    tokenized_corpus = []  # Let us put the tokenized corpus in a list\n",
    "    for sentence in data:\n",
    "        sentence = sentence.lower()\n",
    "        tokenized_sentence = []\n",
    "        for token in sentence.split(' '):  # simplest split is\n",
    "            for tok in token.split('-'):\n",
    "                if tok in stopwords.words('english'):\n",
    "                    continue\n",
    "                tok = re.sub(\"\\d+\", \"\", tok)\n",
    "                tokenized_sentence.append(tok)\n",
    "        tokenized_corpus.append(tokenized_sentence)\n",
    "    # Create single list of all vocabulary\n",
    "    vocabulary = []  # Let us put all the tokens (mostly words) appearing in the vocabulary in a list\n",
    "    for sentence in tokenized_corpus:\n",
    "        for token in sentence:\n",
    "            if token not in vocabulary:\n",
    "                if True:\n",
    "                    vocabulary.append(token)\n",
    "    return vocabulary, tokenized_corpus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word \n",
      "Word chibok\n",
      "Word nagorno\n",
      "Word manafort\n",
      "Word mnuchin\n",
      "Word analytica\n",
      "Word microsecond\n",
      "Word tillerson\n",
      "Word manassian\n",
      "Word hartzler\n",
      "Word nattering\n",
      "Word nabobs\n",
      "Word doddering\n",
      "Word dotards\n",
      "Word deplorableness\n",
      "Word jives\n",
      "Word tipline\n",
      "Word gorsuch\n",
      "Word scalise\n",
      "Word ukraines\n",
      "Word recuses\n",
      "Word halfhearted\n",
      "Word iannucci\n",
      "Word kushners\n",
      "Word brexit\n",
      "Word secessionist\n",
      "Word gravediggers\n",
      "Word pallbearer\n",
      "Word delingpole\n",
      "Word orangeness\n",
      "Word zinke\n",
      "Word matzos\n",
      "Word navajos\n",
      "Word pocohontas\n",
      "Word aecon\n",
      "Word consummated\n",
      "Word hichilema\n",
      "Word equivocation\n",
      "Word canoodles\n",
      "Word #war\n",
      "Word mcenany\n",
      "Word peskov\n",
      "Word nondisclosure\n",
      "Word reenactors\n",
      "Word graveling\n",
      "Word cyberweapons\n",
      "Word nightgowns\n",
      "Word roiled\n",
      "Word warmbier\n",
      "Word careening\n",
      "Word #metoo\n",
      "Word guantรกnamo\n",
      "Word scaramucci\n",
      "Word arbaeen\n",
      "Word lavishes\n",
      "Word infowars.com\n",
      "Word straggled\n",
      "Word myeshia\n",
      "Word falzone\n",
      "Word rescinding\n",
      "Word reanimating\n",
      "Word nakedly\n",
      "Word sympathising\n",
      "Word scrutinising\n",
      "Word deripaska\n",
      "Word npr/ipsos\n",
      "Word rosenstein\n",
      "Word higbie\n",
      "Word roboticists\n",
      "Word dubke\n",
      "Word toupees\n",
      "Word ardern\n",
      "Word sashays\n",
      "Word broaddrick\n",
      "Word sh*t\n",
      "Word fabricates\n",
      "Word maute\n",
      "Word meddled\n",
      "Word !!!!!!!!!!\n",
      "Word fahrenthold\n",
      "Word #womensmarch\n",
      "Word unelectable\n",
      "Word bigly\n",
      "Word counseled\n",
      "Word ironworker\n",
      "Word grigory\n",
      "Word rodchenkov\n",
      "Word blintz\n",
      "Word disinvites\n",
      "Word stupefying\n",
      "Word misinform\n",
      "Word covfefe\n",
      "Word schlapps\n",
      "Word haspel\n",
      "Word cernan\n",
      "Word puzder\n",
      "Word ...\n",
      "Word wiretapped\n",
      "Word redecorates\n",
      "Word immolating\n",
      "Word frexit\n",
      "Word bamboozles\n",
      "Word chauvinists\n",
      "Word delousing\n",
      "Word yazidi\n",
      "Word gianforte\n",
      "Word bagpiping\n",
      "Word mulvaney\n",
      "Word nobost\n",
      "Word declassifies\n",
      "Word kellyanne\n",
      "Word thaad\n",
      "Word transtemporalize\n",
      "Word farenthold\n",
      "Word swalwell\n",
      "Word declassify\n",
      "Word immigrating\n",
      "Word frightbart\n",
      "Word gabbard\n",
      "Word delicatessens\n",
      "Word mispronunciations\n",
      "Word exerpts\n",
      "Word compunction\n",
      "Word wallpapering\n",
      "Word autocrats\n",
      "Word upending\n",
      "Word counterspies\n",
      "Word philandering\n",
      "Word believability\n",
      "Word sweetening\n",
      "Word !!!\n",
      "Word trumpcare\n",
      "Word strzok\n",
      "Word coherency\n",
      "Word hertzberg\n",
      "Word kislyak\n",
      "Word beitar\n",
      "Word abetted\n",
      "Word hunkers\n",
      "Word propagandistic\n",
      "Word hairpieces\n",
      "Word revocations\n",
      "Word extrapolates\n",
      "Word trumper\n",
      "Word gentiloni\n",
      "Word f*ck\n",
      "Word adulting\n",
      "Word dotard\n",
      "Word cathey\n",
      "Word gunmam\n",
      "Word helwan\n",
      "Word weaponizes\n",
      "Word unplumbed\n",
      "Word scuffling\n",
      "Word icbms\n",
      "Word ventral\n",
      "Word sconces\n",
      "Word halloweeners\n",
      "Word greitens\n",
      "Word voterbase\n",
      "Word hastert\n",
      "Word onpolitics\n",
      "Word ventilates\n",
      "Word gofer\n",
      "Word anbang\n",
      "Word fagggots\n",
      "Word nigggers\n",
      "Word undersecretary\n",
      "Word princeling\n",
      "Word regeni\n",
      "Word upwind\n",
      "Word contortions\n",
      "Word exhumes\n",
      "Word nunberg\n",
      "Word roundness\n",
      "Word myopics\n",
      "Word remarries\n",
      "Word mirzakhani\n",
      "Word concocts\n",
      "Word cnnpolitics.com\n",
      "Word isolationism\n",
      "Word shulkin\n",
      "Word defenestrates\n",
      "Word denuclearization\n",
      "Word misplaces\n",
      "Word corncobs\n",
      "Word catawampus\n",
      "Word niebuhr\n",
      "Word trumpist\n",
      "Word scythian\n",
      "Word catalogued\n",
      "Word footrace\n",
      "Word hjiab\n",
      "Word coracobrachialis\n",
      "Word haemorrhoid\n",
      "Word #nomorenazi\n",
      "Word misanthropics\n",
      "Word handedness\n",
      "Word regurgitator\n",
      "Word vellicate\n",
      "Word yoest\n",
      "Word stoneman\n",
      "Word sekulow\n",
      "Word birdlime\n",
      "Word turpitude\n",
      "Word decoders\n",
      "Word populists\n",
      "Word kuaishou\n",
      "Word philando\n",
      "Word exonerating\n",
      "Word eyesores\n",
      "Word dipsomaniac\n",
      "Word nonbinary\n",
      "Word salon.com\n",
      "Word hyperplasia\n",
      "Word swindlers\n",
      "Word psychoneurotic\n",
      "Word bacchanalia\n",
      "Word trumpenomics\n",
      "Word westworld\n",
      "Word crownprince\n",
      "Word grovels\n",
      "Word pussyhats\n",
      "Word malingerer\n",
      "Word pigsties\n",
      "Word dccc\n",
      "Word pompadours\n",
      "Word hoedowns\n",
      "Word @realdonaldtrump\n",
      "Word somnambulist\n",
      "Word creditloan\n",
      "Word puigdemont\n",
      "Word flagellation\n",
      "Word pizzagate\n",
      "Word fakiness\n",
      "Word sychologists\n",
      "Word o’rourke\n",
      "Word cyberwars\n",
      "Word philanderers\n",
      "Word emptive\n",
      "Word impanels\n",
      "Word palindromic\n",
      "Word shkreli\n",
      "Word mypillow\n",
      "Word countermemo\n",
      "Word languishes\n",
      "Word trumpism\n",
      "Word tattling\n",
      "Word anticapitalist\n",
      "Word emasculate\n",
      "Word tweetstorms\n",
      "Word dealmaking\n",
      "Word anthropomorphism\n",
      "Word htin\n",
      "Word kyaw\n",
      "Word eyeballed\n",
      "Word @cnn\n",
      "Word dawdles\n",
      "Word renegotiation\n",
      "Word whitson\n",
      "Word dawdling\n",
      "Word bobridge\n",
      "Word deportees\n",
      "Word nevertrump\n",
      "Word cultivators\n",
      "Word uninsuredrepublican\n",
      "Word dustpans\n",
      "Word frankfurters\n",
      "Word pantomimes\n",
      "Word piroshki\n",
      "Word hemline\n",
      "Word eichenwald\n",
      "Word edentulous\n",
      "Word dematerialize\n",
      "Word netroots\n",
      "Word doghouses\n",
      "Word floggings\n",
      "Word mccarthyite\n",
      "Word boogied\n",
      "Word eructations\n",
      "Word foretells\n",
      "Word sashaying\n",
      "Word cartographers\n",
      "Word gigantism\n",
      "Word noncriminal\n",
      "Word harassments\n",
      "Word agoraphobics\n",
      "Word intoxicates\n",
      "Word centenarian\n",
      "Word deodorize\n",
      "Word shipworm\n",
      "Word falwell\n",
      "Word yiannopoulos\n",
      "Word pomposity\n",
      "Word heightening\n",
      "Word brexiteers\n",
      "Word rollerskater\n",
      "Word recusal\n",
      "Word brayed\n",
      "Word mcgahn\n",
      "Word cornholing\n",
      "Word raptures\n",
      "Word scrubdown\n",
      "Word dismembers\n",
      "Word binomo\n",
      "Word redact\n",
      "Word disinviting\n",
      "Word macedonians\n",
      "Word kompromat\n",
      "Word squawks\n",
      "Word skywriting\n",
      "Word spinelessness\n",
      "Word nursemaid\n",
      "Word prattles\n",
      "Word fascistic\n",
      "Word greasiness\n",
      "Word disrobe\n",
      "Word ejaculations\n",
      "Word kneecapped\n",
      "Word octogenarians\n",
      "Word vagrants\n",
      "Word rohrabacher\n",
      "Word primps\n",
      "Word inebriate\n",
      "Word bordellos\n",
      "Word horoscopists\n",
      "Word chadors\n",
      "Word misandrist\n",
      "Word skripal\n",
      "Word gabbanelli\n",
      "Word cantabella\n",
      "Word pestles\n",
      "Word snuffing\n",
      "Word paraquat\n",
      "Word aerodynamicist\n",
      "Word dressmakers\n",
      "Word fruitworm\n",
      "Word kennelmaster\n",
      "Word gallstone\n",
      "Word debacles\n",
      "Word pussyfoots\n",
      "Word vagaries\n",
      "Word crossbreeding\n",
      "Word kremlins\n",
      "Word cosmetologists\n",
      "Word crookedness\n",
      "Word rutabagas\n",
      "Word oversights\n",
      "Word vomitorium\n",
      "Word oglings\n",
      "Word ussocom\n",
      "Word macdill\n",
      "Word developping\n",
      "Word hindquarters\n",
      "Word macadamias\n",
      "Word rehashes\n",
      "Word irakly\n",
      "Word kaveladze\n",
      "Word coiffuring\n",
      "Word rehires\n",
      "Word detainer\n",
      "Word c&amp;a\n",
      "Word r&amp;d\n",
      "Word kobach\n",
      "Word uncritical\n",
      "Word puddling\n",
      "Word firmest\n",
      "Word featherbrain\n",
      "Word rhinoceroses\n",
      "Word aleksei\n",
      "Word extradites\n",
      "Word fetterman\n",
      "Word unemployables\n",
      "Word lahren\n",
      "Word unpresidential\n",
      "Word murkowski\n",
      "Word emboldens\n",
      "Word befuddle\n",
      "Word spielbe\n",
      "Word avenatti\n",
      "Word homemakers\n",
      "Word bombast\n",
      "Word declassifying\n",
      "Word hauberk\n",
      "Word .............\n",
      "Word overcooking\n",
      "Word analysys\n",
      "Word slived\n",
      "Word tapdanced\n",
      "Word assaulter\n",
      "Word homeplace\n",
      "Word pantsuits\n",
      "Word emoluments\n",
      "Word holdover\n",
      "Word jowls\n",
      "Word scoutmaster\n",
      "Word partitioned\n",
      "Word tapdance\n",
      "Word laundromats\n",
      "Word plods\n",
      "Word guillotines\n",
      "Word wp/abc\n",
      "Word swordsmen\n",
      "Word westernised\n",
      "Word reshuffled\n",
      "Word mansard\n",
      "Word barassing\n",
      "Word sterilizing\n",
      "Word u.s.power\n",
      "Word damore\n",
      "Word cliven\n",
      "Word oppossum\n",
      "Word bootlick\n",
      "Word jukeboxes\n",
      "Word incinerates\n",
      "Word outflank\n",
      "Word invalids\n",
      "Word disrobing\n",
      "Word guffaws\n",
      "Word morticians\n",
      "Word castigates\n",
      "Word pottinger\n",
      "Word fatuousness\n",
      "Word somnambulists\n",
      "Word silkworm\n",
      "Word decontaminate\n",
      "Number of words not in GloVe: 426\n"
     ]
    }
   ],
   "source": [
    "joint_vocab, joint_tokenized_corpus = remove_stopwords(pd.concat([edited_training, edited_test]))\n",
    "\n",
    "_, words_not_in_glove = build_embedding_tensor(joint_vocab, 100)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}