{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import codecs\n",
    "from dataloaders import Task1Dataset, collate_fn_padd, dataset_split\n",
    "\n",
    "from processor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Setting random seed and device\n",
    "SEED = 1\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/task-1/train.csv')\n",
    "test_df = pd.read_csv('data/task-1/dev.csv')\n",
    "\n",
    "training_data = train_df['original']\n",
    "training_edits = train_df['edit']\n",
    "test_data = test_df['original']\n",
    "test_edits = test_df['edit']\n",
    "\n",
    "edited_training = pd.Series(create_edited_sentences(training_data, training_edits))\n",
    "edited_test = pd.Series(create_edited_sentences(test_data, test_edits))\n",
    "\n",
    "# Creating word vectors\n",
    "training_vocab, training_tokenized_corpus = create_vocab(edited_training)\n",
    "test_vocab, test_tokenized_corpus = create_vocab(edited_test)\n",
    "joint_vocab, joint_tokenized_corpus = create_vocab(pd.concat([edited_training, edited_test]))\n",
    "\n",
    "word2idx = get_word2idx(joint_vocab)\n",
    "\n",
    "training_grades = train_df['meanGrade']\n",
    "\n",
    "training_vector_sentences = vectorize_sentences(training_tokenized_corpus, word2idx)\n",
    "training_dataset = Task1Dataset(training_vector_sentences, training_grades)\n",
    "\n",
    "train_dataset, validation_dataset = dataset_split(training_dataset)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn_padd)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def model_performance(output, target, print_output=False):\n",
    "    \"\"\"\n",
    "    Returns SSE and MSE per batch (printing the MSE and the RMSE)\n",
    "    \"\"\"\n",
    "\n",
    "    sq_error = (output - target)**2\n",
    "\n",
    "    sse = np.sum(sq_error)\n",
    "    mse = np.mean(sq_error)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    if print_output:\n",
    "        print(f'| MSE: {mse:.2f} | RMSE: {rmse:.2f} |')\n",
    "\n",
    "    return sse, mse\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
    "        super(FFNN, self).__init__()\n",
    "        # embedding (lookup layer) layer\n",
    "        # padding_idx argument makes sure that the 0-th token in the vocabulary\n",
    "        # is used for padding purposes i.e. its embedding will be a 0-vector\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        # hidden layer\n",
    "        self.fc1 = nn.Linear(embedding_dim, 20)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(10, 5)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        # output layer\n",
    "        self.output = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        sentence_lengths = x.ne(0).sum(1, keepdims=True)\n",
    "        averaged = embedded.sum(1) / sentence_lengths\n",
    "        out = self.fc1(averaged)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.output(out)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def eval(data_iter, model):\n",
    "    \"\"\"\n",
    "    Evaluating model performance on the dev set\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_sse = 0\n",
    "    pred_all = []\n",
    "    trg_all = []\n",
    "    no_observations = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_iter:\n",
    "            feature, target = batch\n",
    "            feature, target = feature.to(device), target.to(device)\n",
    "            # for RNN:\n",
    "            # model.batch_size = target.shape[0]\n",
    "            no_observations = no_observations + target.shape[0]\n",
    "            # model.hidden = model.init_hidden()\n",
    "\n",
    "            predictions = model(feature).squeeze(1)\n",
    "            loss = loss_fn(predictions, target)\n",
    "\n",
    "            # We get the mse\n",
    "            pred, trg = predictions.detach().cpu().numpy(), target.detach().cpu().numpy()\n",
    "            sse, __ = model_performance(pred, trg)\n",
    "\n",
    "            epoch_loss += loss.item()*target.shape[0]\n",
    "            epoch_sse += sse\n",
    "            pred_all.extend(pred)\n",
    "            trg_all.extend(trg)\n",
    "\n",
    "    return epoch_loss/no_observations, epoch_sse/no_observations, np.array(pred_all), np.array(trg_all)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50\n",
    "HIDDEN_DIM = 50\n",
    "model = FFNN(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx))\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def train(train_loader, validation_loader, model, number_epoch):\n",
    "    \"\"\"\n",
    "    Training loop for the model, which calls on eval to evaluate after each epoch\n",
    "    \"\"\"\n",
    "    print(\"Training model.\")\n",
    "    for epoch in range(1, number_epoch+1):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_sse = 0\n",
    "        no_observations = 0  # Observations used for training so far\n",
    "        for batch in train_loader:\n",
    "            feature, target = batch\n",
    "            feature, target = feature.to(device), target.to(device)\n",
    "            # for RNN:\n",
    "            # model.batch_size = target.shape[0]\n",
    "            no_observations = no_observations + target.shape[0]\n",
    "            # model.hidden = model.init_hidden()\n",
    "            predictions = model(feature).squeeze(1)\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(predictions, target)\n",
    "            sse, __ = model_performance(predictions.detach().cpu().numpy(), target.detach().cpu().numpy())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()*target.shape[0]\n",
    "            epoch_sse += sse\n",
    "\n",
    "        valid_loss, valid_mse, __, __ = eval(validation_loader, model)\n",
    "\n",
    "        epoch_loss, epoch_mse = epoch_loss / no_observations, epoch_sse / no_observations\n",
    "        print(f'| Epoch: {epoch:02} | Train Loss: {epoch_loss:.2f} | Train MSE: {epoch_mse:.2f} | Train RMSE: {epoch_mse**0.5:.2f} | \\\n",
    "        Val. Loss: {valid_loss:.2f} | Val. MSE: {valid_mse:.2f} |  Val. RMSE: {valid_mse**0.5:.2f} |')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n",
      "| Epoch: 01 | Train Loss: 0.50 | Train MSE: 0.50 | Train RMSE: 0.70 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.58 |\n",
      "| Epoch: 02 | Train Loss: 0.33 | Train MSE: 0.33 | Train RMSE: 0.58 |         Val. Loss: 0.33 | Val. MSE: 0.33 |  Val. RMSE: 0.58 |\n",
      "| Epoch: 03 | Train Loss: 0.32 | Train MSE: 0.32 | Train RMSE: 0.57 |         Val. Loss: 0.33 | Val. MSE: 0.33 |  Val. RMSE: 0.58 |\n",
      "| Epoch: 04 | Train Loss: 0.31 | Train MSE: 0.31 | Train RMSE: 0.55 |         Val. Loss: 0.33 | Val. MSE: 0.33 |  Val. RMSE: 0.58 |\n",
      "| Epoch: 05 | Train Loss: 0.29 | Train MSE: 0.29 | Train RMSE: 0.53 |         Val. Loss: 0.34 | Val. MSE: 0.34 |  Val. RMSE: 0.58 |\n",
      "| Epoch: 06 | Train Loss: 0.26 | Train MSE: 0.26 | Train RMSE: 0.51 |         Val. Loss: 0.35 | Val. MSE: 0.35 |  Val. RMSE: 0.59 |\n",
      "| Epoch: 07 | Train Loss: 0.23 | Train MSE: 0.23 | Train RMSE: 0.48 |         Val. Loss: 0.36 | Val. MSE: 0.36 |  Val. RMSE: 0.60 |\n",
      "| Epoch: 08 | Train Loss: 0.21 | Train MSE: 0.21 | Train RMSE: 0.45 |         Val. Loss: 0.37 | Val. MSE: 0.37 |  Val. RMSE: 0.61 |\n",
      "| Epoch: 09 | Train Loss: 0.18 | Train MSE: 0.18 | Train RMSE: 0.43 |         Val. Loss: 0.39 | Val. MSE: 0.39 |  Val. RMSE: 0.62 |\n",
      "| Epoch: 10 | Train Loss: 0.16 | Train MSE: 0.16 | Train RMSE: 0.40 |         Val. Loss: 0.40 | Val. MSE: 0.40 |  Val. RMSE: 0.63 |\n",
      "| Epoch: 11 | Train Loss: 0.14 | Train MSE: 0.14 | Train RMSE: 0.38 |         Val. Loss: 0.41 | Val. MSE: 0.41 |  Val. RMSE: 0.64 |\n",
      "| Epoch: 12 | Train Loss: 0.12 | Train MSE: 0.12 | Train RMSE: 0.35 |         Val. Loss: 0.42 | Val. MSE: 0.42 |  Val. RMSE: 0.65 |\n",
      "| Epoch: 13 | Train Loss: 0.11 | Train MSE: 0.11 | Train RMSE: 0.33 |         Val. Loss: 0.44 | Val. MSE: 0.44 |  Val. RMSE: 0.66 |\n",
      "| Epoch: 14 | Train Loss: 0.10 | Train MSE: 0.10 | Train RMSE: 0.31 |         Val. Loss: 0.44 | Val. MSE: 0.44 |  Val. RMSE: 0.66 |\n",
      "| Epoch: 15 | Train Loss: 0.09 | Train MSE: 0.09 | Train RMSE: 0.29 |         Val. Loss: 0.45 | Val. MSE: 0.45 |  Val. RMSE: 0.67 |\n",
      "| Epoch: 16 | Train Loss: 0.08 | Train MSE: 0.08 | Train RMSE: 0.28 |         Val. Loss: 0.46 | Val. MSE: 0.46 |  Val. RMSE: 0.68 |\n",
      "| Epoch: 17 | Train Loss: 0.07 | Train MSE: 0.07 | Train RMSE: 0.26 |         Val. Loss: 0.48 | Val. MSE: 0.48 |  Val. RMSE: 0.69 |\n",
      "| Epoch: 18 | Train Loss: 0.06 | Train MSE: 0.06 | Train RMSE: 0.24 |         Val. Loss: 0.49 | Val. MSE: 0.49 |  Val. RMSE: 0.70 |\n",
      "| Epoch: 19 | Train Loss: 0.05 | Train MSE: 0.05 | Train RMSE: 0.23 |         Val. Loss: 0.50 | Val. MSE: 0.50 |  Val. RMSE: 0.71 |\n",
      "| Epoch: 20 | Train Loss: 0.05 | Train MSE: 0.05 | Train RMSE: 0.22 |         Val. Loss: 0.51 | Val. MSE: 0.51 |  Val. RMSE: 0.71 |\n"
     ]
    }
   ],
   "source": [
    "train(train_loader, validation_loader, model, 20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}